{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## tesseract - doctr setup"
      ],
      "metadata": {
        "id": "Tc3rEZzeXRxe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrgLjqtUTQLE"
      },
      "outputs": [],
      "source": [
        "# Tesseract OCR\n",
        "print(\"Tesseract OCR setup...\")\n",
        "!apt-get update > /dev/null\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-tur > /dev/null\n",
        "print(\"Tesseract setup done.\")\n",
        "\n",
        "print(\"Python libraries\")\n",
        "!pip install \"python-doctr[torch]\" pytesseract tqdm -q\n",
        "print(\"Library set up done.\")\n",
        "\n",
        "# other required dependencies\n",
        "!pip install opencv-python-headless pytesseract numpy matplotlib\n",
        "!sudo apt update\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-tur"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "BMJDEZgVShfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pytesseract\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import glob\n",
        "import torch\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from typing import List, Dict\n",
        "from pytesseract import image_to_data, Output\n",
        "from tqdm.notebook import tqdm\n",
        "from skimage.filters import threshold_sauvola\n",
        "from scipy.ndimage import rotate\n",
        "from difflib import get_close_matches\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files, drive  # For file uploads and Google Drive mounting\n",
        "from google.colab.patches import cv2_imshow  # For displaying images in Colab\n",
        "import cv2\n",
        "# from skimage.filters import threshold_sauvola\n",
        "from PIL import Image\n",
        "# Import doctr for OCR\n",
        "from doctr.io import DocumentFile\n",
        "from doctr.models import ocr_predictor, db_resnet50, detection_predictor\n",
        "import tempfile\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from sklearn.cluster import DBSCAN\n",
        "print(\"Installed dependencies.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsrsoZj9TUPY",
        "outputId": "afcdfa2c-1d95-4016-997c-94370f38b3dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed dependencies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG9wGYQATWJi",
        "outputId": "cbbf2a7b-05ed-4dfc-897f-4350a98fcfef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **`load_doctr_model()`** loads the Doctr OCR model **(ocr_predictor)** with detection architecture **DB-ResNet50** and recognition architecture **CRNN-VGG16-BN**.\n",
        "- The model is set to evaluation mode and returned for use in OCR tasks."
      ],
      "metadata": {
        "id": "Us8IYqOESVI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_doctr_model():\n",
        "    \"\"\"\n",
        "    Load the Doctr OCR model.\n",
        "\n",
        "    Returns:\n",
        "        model (doctr.models.ocr_predictor): The loaded OCR predictor model in evaluation mode.\n",
        "                                             Returns None if loading fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        _device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        model = ocr_predictor(\n",
        "            pretrained=True,\n",
        "            det_arch='db_resnet50',\n",
        "            reco_arch='crnn_vgg16_bn'\n",
        "        ).to(_device)\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        print(f\"Doctr OCR model loaded on {_device}.\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Doctr model could not be loaded: {e}. OCR functionality will be limited.\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "JS1AsVb_U9BM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doctr_model = load_doctr_model()"
      ],
      "metadata": {
        "id": "_kFqzfQejy5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce4d1349-fc2c-464b-97f5-11938f95381d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doctr OCR model loaded on cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sem_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D75c1027Us1",
        "outputId": "94711cc3-69bb-4903-9d36-7f26be70abd8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# keywords for the receipt detection\n",
        "keywords_for_detection = [\n",
        "    'TOPLAM', 'TUTAR', 'KDV', 'TL', 'FATURA', 'TARİH', 'SAAT', 'İŞLEM', 'MÜŞTERİ', 'BAŞARILI',\n",
        "    'SATIŞ', 'TUTARI', 'YERMİNAL', 'GARANTİ', 'ONAY', 'MERSIS', 'KART', 'BANKA', 'NO', 'ADET',\n",
        "    'ISLEM', 'PARAMETRE', 'YUKLEME', 'TERMİNAL', 'ISYERİ', 'TERMINAL', 'BATCH', 'GARANTI BBVA',\n",
        "    'DENIZBANK', 'TOPKDV', '1,00 TL', 'SATIŞ TUTARI', 'İŞLEM TUTARI','IPTAL','İPTAL','ISYERI'\n",
        "]"
      ],
      "metadata": {
        "id": "j6RvEGQyU_e4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This module below makes sure each receipt image is upright and deskewed before OCR (Doctr). It first fixes coarse rotation (0/90/180/270), then corrects small tilt, and optionally fine-refines alignment."
      ],
      "metadata": {
        "id": "4KVuOhuEZS2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_image_precise(image, angle, background_color=(255, 255, 255)):\n",
        "    \"\"\"\n",
        "    Why used:\n",
        "      - Apply arbitrary-angle rotation WITHOUT cropping, so no text is lost.\n",
        "      - Used for both coarse (0/90/180/270) and fine skew corrections before OCR.\n",
        "    \"\"\"\n",
        "    if angle == 0: return image\n",
        "    h, w = image.shape[:2]\n",
        "    center = (w // 2, h // 2)\n",
        "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    cos, sin = np.abs(M[0, 0]), np.abs(M[0, 1])\n",
        "    nW, nH = int((h * sin) + (w * cos)), int((h * cos) + (w * sin))\n",
        "    M[0, 2] += (nW / 2) - center[0]\n",
        "    M[1, 2] += (nH / 2) - center[1]\n",
        "    return cv2.warpAffine(image, M, (nW, nH), flags=cv2.INTER_CUBIC,\n",
        "                          borderMode=cv2.BORDER_CONSTANT, borderValue=background_color)\n",
        "\n",
        "def estimate_skew_hough(image):\n",
        "    \"\"\"\n",
        "    Why used:\n",
        "      - After coarse rotation, receipts may still be slightly tilted.\n",
        "      - Detects small skew via Hough lines and returns a robust median angle.\n",
        "    \"\"\"\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    inverted = cv2.bitwise_not(gray)\n",
        "    thresh = cv2.adaptiveThreshold(inverted, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                   cv2.THRESH_BINARY, 15, 2)\n",
        "    dilated = cv2.dilate(thresh, np.ones((3, 3), np.uint8), iterations=1)\n",
        "    edges = cv2.Canny(dilated, 50, 150)\n",
        "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 150,\n",
        "                            minLineLength=100, maxLineGap=20)\n",
        "    angles = []\n",
        "    if lines is not None:\n",
        "        for x1, y1, x2, y2 in lines[:, 0]:\n",
        "            if x2 == x1: continue\n",
        "            angle = np.degrees(np.arctan2(y2 - y1, x2 - x1))\n",
        "            if -45 < angle < 45:\n",
        "                angles.append(angle)\n",
        "    return np.median(angles) if angles else 0\n",
        "\n",
        "def projection_variance_score(img):\n",
        "    \"\"\"\n",
        "    Why used:\n",
        "      - Cheap numeric proxy for “how straight” text lines are.\n",
        "      - Enables micro-refinement by comparing nearby angles (higher = better).\n",
        "    \"\"\"\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    inverted = cv2.bitwise_not(thresh)\n",
        "    proj = np.sum(inverted, axis=1)\n",
        "    return np.var(proj)\n",
        "\n",
        "def compute_avg_conf(data):\n",
        "    \"\"\"\n",
        "    Why used:\n",
        "      - Orientation selection needs an OCR quality signal.\n",
        "      - Average token confidence is a stable metric to compare rotations.\n",
        "    \"\"\"\n",
        "    confs = []\n",
        "    for txt, conf in zip(data['text'], data['conf']):\n",
        "        try:\n",
        "            val = float(conf)\n",
        "            if txt.strip() and 0 < val < 100:\n",
        "                confs.append(val)\n",
        "        except:\n",
        "            continue\n",
        "    return np.mean(confs) if confs else 0\n",
        "\n",
        "def count_keywords(text, keywords):\n",
        "    \"\"\"\n",
        "    Why used:\n",
        "      - Receipts contain domain-specific tokens (TOPLAM, ONAY, etc.).\n",
        "      - Acts as a prior to break ties when OCR confidences are close.\n",
        "    \"\"\"\n",
        "    text = text.upper()\n",
        "    return sum(text.count(k.upper()) for k in keywords)\n",
        "\n",
        "def find_best_orientation(image, keywords, lang='tur'):\n",
        "    \"\"\"\n",
        "    Why used:\n",
        "      - Doctr is rotation-sensitive; choose best of 0/90/180/270 FIRST.\n",
        "      - Scores each angle by (keyword hits + OCR confidence), with a small angle penalty,\n",
        "        then returns the top candidate to feed into skew correction.\n",
        "    \"\"\"\n",
        "    best_img = image\n",
        "    best_angle = 0\n",
        "    best_score = -1\n",
        "    best_conf = 0\n",
        "    best_kw_count = 0\n",
        "\n",
        "    angle_penalty = {\n",
        "        0: 1.0,\n",
        "        90: 0.85,\n",
        "        180: 0.7,\n",
        "        270: 0.85\n",
        "    }\n",
        "\n",
        "    for angle in [0, 90, 180, 270]:\n",
        "        rotated = rotate_image_precise(image, angle)\n",
        "        data = pytesseract.image_to_data(rotated, lang=lang, output_type=Output.DICT)\n",
        "        conf = compute_avg_conf(data)\n",
        "        text = \" \".join(data['text'])\n",
        "        kw_count = count_keywords(text, keywords)\n",
        "\n",
        "        # Apply penalty to prefer natural upright when signals tie\n",
        "        base_score = kw_count * 1000 + conf\n",
        "        final_score = base_score * angle_penalty.get(angle, 0.7)\n",
        "\n",
        "        if final_score > best_score:\n",
        "            best_score = final_score\n",
        "            best_img = rotated\n",
        "            best_angle = angle\n",
        "            best_conf = conf\n",
        "            best_kw_count = kw_count\n",
        "\n",
        "    return best_img, best_angle\n",
        "\n",
        "def correct_orientation_with_skew(image, keywords=None, lang='tur+eng',\n",
        "                                  skew_range=5, step=1.0, top_n_angles=3):\n",
        "    \"\"\"\n",
        "    Why used:\n",
        "      - Single entry point to produce a Doctr-ready image.\n",
        "      - Pipeline: coarse rotation → small skew estimation → optional micro-refine.\n",
        "      - Returns final image + total angle for audit/reproducibility.\n",
        "    \"\"\"\n",
        "    # Step 1: Try 0°, 90°, 180°, 270°\n",
        "    upright_img, base_angle = find_best_orientation(image, keywords, lang=lang)\n",
        "\n",
        "    # Step 2: Estimate fine skew angle\n",
        "    skew_angle = estimate_skew_hough(upright_img)\n",
        "    corrected_img = rotate_image_precise(upright_img, skew_angle)\n",
        "\n",
        "    # Step 3: Optionally refine skew using projection variance\n",
        "    # print(\"Refining skew...\")\n",
        "    angles = np.arange(skew_angle - skew_range, skew_angle + skew_range + step, step)\n",
        "    scored_angles = []\n",
        "    for a in angles:\n",
        "        r = rotate_image_precise(upright_img, a)\n",
        "        score = projection_variance_score(r)\n",
        "        scored_angles.append((a, score))\n",
        "\n",
        "    top_angles = sorted(scored_angles, key=lambda x: -x[1])[:top_n_angles]\n",
        "    best_conf = compute_avg_conf(pytesseract.image_to_data(corrected_img, lang=lang, output_type=Output.DICT))\n",
        "    best_img = corrected_img\n",
        "    best_angle = base_angle + skew_angle\n",
        "\n",
        "    for a, _ in top_angles:\n",
        "        rotated = rotate_image_precise(upright_img, a)\n",
        "        data = pytesseract.image_to_data(rotated, lang=lang, output_type=Output.DICT)\n",
        "        conf = compute_avg_conf(data)\n",
        "        if conf > best_conf:\n",
        "            best_conf = conf\n",
        "            best_img = rotated\n",
        "            best_angle = base_angle + a\n",
        "\n",
        "    return best_img, best_angle"
      ],
      "metadata": {
        "id": "8Aab8Y4mZFHP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `apply_light_clahe(image_bgr)`\n",
        "**Why used:** Enhance text visibility and contrast without over-saturating the image, especially in low-contrast or faded receipts.  \n",
        "**How it works:**\n",
        "1. **Convert to LAB:** Separates luminance (L) from color channels (A, B).  \n",
        "2. **CLAHE on L-channel:** Applies *Contrast Limited Adaptive Histogram Equalization* with a low clip limit (≤3.0) to subtly boost local contrast.  \n",
        "3. **Merge & Convert:** Combines adjusted luminance with original color and converts back to BGR.  \n",
        "\n",
        "**Returns:** A contrast-enhanced BGR image, ready for further preprocessing or OCR."
      ],
      "metadata": {
        "id": "zujqzFIBZd2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_light_clahe(image_bgr):\n",
        "    # Convert to LAB color space\n",
        "    lab = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "\n",
        "    # Apply CLAHE to the L-channel\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))  # clipLimit < 3.0 is \"light\"\n",
        "    cl = clahe.apply(l)\n",
        "\n",
        "    # Merge and convert back\n",
        "    merged = cv2.merge((cl, a, b))\n",
        "    enhanced = cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    return enhanced"
      ],
      "metadata": {
        "id": "5xWjGCM3TWcF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `cluster_receipts_by_centroids` — Purpose\n",
        "\n",
        "This function groups detected OCR words into **clusters** (likely representing individual receipts)  \n",
        "based on their **center coordinates** in the image.  \n",
        "It uses **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) to find  \n",
        "groups of nearby words without needing to predefine the number of clusters.\n",
        "\n",
        "#### Why this is needed\n",
        "- A single image can contain **multiple receipts**.\n",
        "- We want to group words that **belong to the same receipt**.\n",
        "- DBSCAN is ideal because it:\n",
        "  - Clusters based on **spatial proximity** (word positions).\n",
        "  - Can handle **noise/outliers** (unwanted words outside receipts).\n",
        "  - Does not require specifying the number of receipts beforehand.\n",
        "\n",
        "#### How it works\n",
        "1. Takes the **center points** of all OCR-detected words.\n",
        "2. Converts `eps_frac` into an actual pixel distance using the image diagonal.\n",
        "3. Runs DBSCAN to find clusters.\n",
        "4. Returns each cluster as a list of word dictionaries.\n"
      ],
      "metadata": {
        "id": "3B7w5s4l58WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_receipts_by_centroids(words, image_shape, eps_frac=0.05, min_samples=5):\n",
        "    \"\"\"\n",
        "    words: list of dicts, each with 'center':(cx,cy), and 'box':(x1,y1,x2,y2)\n",
        "    image_shape: (h,w) of the image\n",
        "    Returns: List of clusters, each a list of word dicts\n",
        "    \"\"\"\n",
        "    if not words:\n",
        "        return []\n",
        "\n",
        "    # build matrix of (cx, cy)\n",
        "    X = np.array([w['center'] for w in words], dtype=float)\n",
        "    # eps = fraction of image diagonal\n",
        "    h, w = image_shape\n",
        "    eps = np.hypot(w, h) * eps_frac\n",
        "\n",
        "    db = DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
        "    labels = db.labels_\n",
        "\n",
        "    clusters = []\n",
        "    for lab in sorted(set(labels)):\n",
        "        if lab < 0:  # noise\n",
        "            continue\n",
        "        idxs = np.where(labels == lab)[0]\n",
        "        cluster = [words[i] for i in idxs]\n",
        "        clusters.append(cluster)\n",
        "    return clusters"
      ],
      "metadata": {
        "id": "PMsJwmZdR3ES"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `compute_receipt_score` — Purpose\n",
        "\n",
        "This function below calculates a **confidence score** to decide if a given cluster of OCR lines is likely to be a **receipt**.\n",
        "\n",
        "#### Why this is needed\n",
        "- After clustering OCR words into potential receipts, we still need to **validate** if a cluster is truly a receipt.\n",
        "- A high keyword match alone may not be enough; receipts also have **amounts, dates, and times**.\n",
        "- This scoring approach gives a **weighted evaluation** instead of a simple yes/no check.\n",
        "\n",
        "#### How it works\n",
        "1. **Combine all text** from the cluster into a single string.\n",
        "2. **Normalize text** for consistent matching (e.g., removing accents, fixing OCR inconsistencies).\n",
        "3. **Count keyword matches** from `keywords_for_detection`.\n",
        "4. **Detect patterns** using regex\n",
        "5. **Calculate weighted score**:\n",
        "6. **Return results** with:\n",
        "   - Score\n",
        "   - Pattern presence booleans\n",
        "   - Final `is_receipt` flag (score ≥ threshold)."
      ],
      "metadata": {
        "id": "EEkrbSd56QLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_receipt_score(\n",
        "    line_texts,\n",
        "    keywords_for_detection,\n",
        "    kw_weight=0.4,\n",
        "    amt_weight=0.6,\n",
        "    date_weight=0.5,\n",
        "    time_weight=0.4,\n",
        "    threshold=1.8,\n",
        "    return_debug=False,\n",
        "    print_debug=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    line_texts: list[str] (one cluster's lines)\n",
        "    keywords_for_detection: list[str]\n",
        "    Returns: dict with score, booleans, is_receipt, and (optionally) debug info.\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    # 1) Flatten + normalize once\n",
        "    full = \" \".join(line_texts or [])\n",
        "    full_norm = normalize_ocr_text(full)\n",
        "\n",
        "    # 2) Keyword hits (count) + which keywords matched\n",
        "    norm_kws = [normalize_ocr_text(k) for k in (keywords_for_detection or [])]\n",
        "    matched_kws = sorted({k for k in norm_kws if k and k in full_norm})\n",
        "    kw_hits = sum(full_norm.count(k) for k in matched_kws)\n",
        "\n",
        "    # 3) Regex signals (+ capture examples)\n",
        "    amount_re = re.compile(r'\\b\\d+[.,]\\d{2}\\s*(?:TL|TRY)?\\b')\n",
        "    date_re   = re.compile(r'\\b\\d{2}[./-]\\d{2}[./-]\\d{2,4}\\b')\n",
        "    time_re   = re.compile(r'\\b\\d{2}[:.]\\d{2}(?::\\d{2})?\\b')\n",
        "\n",
        "    amount_matches = amount_re.findall(full_norm)\n",
        "    date_matches   = date_re.findall(full_norm)\n",
        "    time_matches   = time_re.findall(full_norm)\n",
        "\n",
        "    has_amount = bool(amount_matches)\n",
        "    has_date   = bool(date_matches)\n",
        "    has_time   = bool(time_matches)\n",
        "\n",
        "    # 4) Score (with visible component contributions)\n",
        "    kw_contrib   = kw_weight  * kw_hits\n",
        "    amt_contrib  = amt_weight * int(has_amount)\n",
        "    date_contrib = date_weight* int(has_date)\n",
        "    time_contrib = time_weight* int(has_time)\n",
        "\n",
        "    score = kw_contrib + amt_contrib + date_contrib + time_contrib\n",
        "    is_receipt = score >= threshold\n",
        "\n",
        "    result = {\n",
        "        \"score\": round(score, 3),\n",
        "        \"kw_hits\": int(kw_hits),\n",
        "        \"has_amount\": bool(has_amount),\n",
        "        \"has_date\": bool(has_date),\n",
        "        \"has_time\": bool(has_time),\n",
        "        \"is_receipt\": is_receipt,\n",
        "    }\n",
        "\n",
        "    if return_debug:\n",
        "        result[\"debug\"] = {\n",
        "            \"text_preview\": full_norm[:300],\n",
        "            \"matched_keywords\": matched_kws,\n",
        "            \"amount_matches\": amount_matches[:10],\n",
        "            \"date_matches\": date_matches[:10],\n",
        "            \"time_matches\": time_matches[:10],\n",
        "            \"weights\": {\n",
        "                \"kw_weight\": kw_weight,\n",
        "                \"amt_weight\": amt_weight,\n",
        "                \"date_weight\": date_weight,\n",
        "                \"time_weight\": time_weight,\n",
        "                \"threshold\": threshold,\n",
        "            },\n",
        "            \"components\": {\n",
        "                \"kw_contrib\": round(kw_contrib, 3),\n",
        "                \"amt_contrib\": round(amt_contrib, 3),\n",
        "                \"date_contrib\": round(date_contrib, 3),\n",
        "                \"time_contrib\": round(time_contrib, 3),\n",
        "            },\n",
        "        }\n",
        "\n",
        "    if print_debug:\n",
        "        print(\n",
        "            f\"[receipt_score] score={score:.3f} (kw={kw_contrib:.2f}, amt={amt_contrib:.2f}, \"\n",
        "            f\"date={date_contrib:.2f}, time={time_contrib:.2f}) | \"\n",
        "            f\"kw_hits={kw_hits} | amount={has_amount} date={has_date} time={has_time} | \"\n",
        "            f\"thr={threshold} => is_receipt={is_receipt}\"\n",
        "        )\n",
        "        if matched_kws:\n",
        "            print(\"  matched_keywords:\", matched_kws)\n",
        "        if amount_matches or date_matches or time_matches:\n",
        "            print(\"  samples:\",\n",
        "                  {\"amount\": amount_matches[:3], \"date\": date_matches[:3], \"time\": time_matches[:3]})\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "m95kkMe6ELuI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adaptive_dbscan_params(words, H, W):\n",
        "    # Image diagonal\n",
        "    diag = (H**2 + W**2) ** 0.5\n",
        "    n_words = len(words)\n",
        "\n",
        "    if n_words == 0:\n",
        "        return 0.08, 5  # fallback\n",
        "\n",
        "    # Estimate spacing from word centers\n",
        "    centers = np.array([w[\"center\"] for w in words])\n",
        "    if len(centers) > 1:\n",
        "        from sklearn.neighbors import NearestNeighbors\n",
        "        nbrs = NearestNeighbors(n_neighbors=2).fit(centers)\n",
        "        distances, _ = nbrs.kneighbors(centers)\n",
        "        avg_dist = np.mean(distances[:, 1])\n",
        "    else:\n",
        "        avg_dist = diag * 0.05\n",
        "\n",
        "    # eps fraction based on spacing (clamp between 0.05 and 0.15)\n",
        "    eps_frac = min(0.15, max(0.05, avg_dist / diag * 1.5))\n",
        "\n",
        "    # min_samples based on total word count\n",
        "    if n_words < 30:\n",
        "        min_samples = 3\n",
        "    elif n_words < 80:\n",
        "        min_samples = 5\n",
        "    else:\n",
        "        min_samples = 8\n",
        "\n",
        "    return eps_frac, min_samples\n"
      ],
      "metadata": {
        "id": "qCODJJn5oqni"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Keep your existing dict but lowercase all entries\n",
        "RECEIPT_TYPE_KEYWORDS = {\n",
        "    \"gunsonu\": [\n",
        "        \"günsonu raporu\",\"günsonu işlemi\",\"rapor başlangıcı\",\"rapor sonu\",\n",
        "        \"başarılı olarak tamamlanmıştır\",\"gönderim raporu\",\"genel toplam\",\n",
        "        \"peşin iptal\",\"peşin ipt\",\"batch no\"\n",
        "    ],\n",
        "    \"satis\": [\n",
        "        \"satış\",\"satış tutarı\",\"işlem tutarı\",\"kredi kartı\",\"onay numarası\",\n",
        "        \"onay kodu\",\"kart sahibine aittir\",\"topkdv\",\"toplam\",\"1,00\",\"1,00tl\",\"i:\",\"t:\"\n",
        "    ],\n",
        "    \"iptal\": [\n",
        "        \"satış iptal\",\"satış iptal tutarı\",\"iptal\",\"ref no\",\"onay numarası\",\n",
        "        \"tarih\",\"1,00tl\",\"kart sahibine aittir\",\"i:\",\"t:\"\n",
        "    ],\n",
        "    \"parametre\": [\n",
        "        \"parametre yükleme\",\"parametre\",\"host\",\"tid\",\"mid\",\"os ver\",\n",
        "        \"başlangıç\",\"bitiş\",\"pos aktivasyon başarılı\",\"key exchange başarılı\"\n",
        "    ],\n",
        "    \"detay_listesi\": [\n",
        "        \"detay işlemler listesi\",\"grup 1 işlem sayısı\",\"grup başarılı\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "AMOUNT_RE = re.compile(r'\\b\\d{1,4}[.,]\\d{2}\\s*(?:TL|TRY)?\\b')\n",
        "DATE_RE   = re.compile(r'\\b\\d{2}[./-]\\d{2}[./-]\\d{2,4}\\b')\n",
        "TIME_RE   = re.compile(r'\\b\\d{2}[:.]\\d{2}(?::\\d{2})?\\b')"
      ],
      "metadata": {
        "id": "AdPs9blMcAX_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `score_cluster_type` — Purpose\n",
        "\n",
        "This function determines the **most likely receipt type** for a given cluster of OCR-extracted lines by combining **keyword-based** and **structural** pattern analysis.\n",
        "\n",
        "#### Why this is needed\n",
        "- A single image can contain **different receipt formats** (e.g., *satış*, *iptal*, *günsonu*, *parametre*).\n",
        "- We need a **type classification step** so downstream processing can apply type-specific extraction rules.\n",
        "- This approach uses **both text keywords** and **receipt-structure patterns** for higher accuracy.\n",
        "\n",
        "#### How it works\n",
        "1. **Text Normalization**\n",
        "   - Converts all text to lowercase, removes accents, and collapses spaces for consistent matching.\n",
        "\n",
        "2. **Keyword Hit Counting**\n",
        "   - For each known receipt type (from `RECEIPT_TYPE_KEYWORDS_NORM`), counts how many of its normalized keywords appear in the cluster.\n",
        "\n",
        "3. **Structural Pattern Boosts**\n",
        "   - Checks for additional receipt-specific patterns:\n",
        "     - Amounts (e.g., `1,00 TL`)\n",
        "     - Dates\n",
        "     - Approval codes (\"Onay numarası\")\n",
        "     - Reference numbers\n",
        "     - Special tags like `I:` or `T:` with digits.\n",
        "   - Adds weighted boosts to *satış* and *iptal* types based on these patterns.\n",
        "\n",
        "4. **Conflict Resolution Rules**\n",
        "   - Penalizes *satış* score if \"iptal\" appears.\n",
        "   - Penalizes *satış*, *iptal*, and *parametre* if *günsonu* signals dominate.\n",
        "   - Penalizes *satış*, *iptal*, and *günsonu* if *parametre* signals dominate.\n",
        "\n",
        "5. **Final Decision**\n",
        "   - Selects the highest-scoring type as `best_type`.\n",
        "   - Also lists all plausible `tags` within a score delta (`delta = 0.6`).\n",
        "   - Returns:\n",
        "     - `type` → most probable receipt type\n",
        "     - `scores` → score per type\n",
        "     - `confidence` → normalized confidence (0-1)\n",
        "     - `tags` → all candidate types above the threshold"
      ],
      "metadata": {
        "id": "XekYCxDl6eoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_ocr_text(text: str) -> str:\n",
        "    # 1) Uppercase\n",
        "    text = text.upper()\n",
        "    # 2) Remove accents (İ -> I, Ş -> S, etc.)\n",
        "    text = unicodedata.normalize(\"NFKD\", text)\n",
        "    text = \"\".join([c for c in text if not unicodedata.combining(c)])\n",
        "    # 3) Replace multiple spaces with single\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    # 4) Strip edges\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "M5Izg-Fo-yjL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _norm(s: str) -> str:\n",
        "    # your normalize_ocr_text: upper + strip accents + collapse spaces\n",
        "    s = normalize_ocr_text(s)\n",
        "    return s.lower()\n",
        "\n",
        "# Build a normalized copy ONCE at import time\n",
        "RECEIPT_TYPE_KEYWORDS_NORM = {\n",
        "    t: [_norm(kw) for kw in kws]\n",
        "    for t, kws in RECEIPT_TYPE_KEYWORDS.items()\n",
        "}\n",
        "\n",
        "I_COLON_RE = re.compile(r'\\b[i1]\\s*:\\s*\\d+')   # I:/1: followed by digits\n",
        "T_COLON_RE = re.compile(r'\\b[t7]\\s*:\\s*\\d+')   # T:/7: followed by digits\n",
        "REFNO_RE   = re.compile(r'\\bref(?:erans)?\\s*no\\b')  # avoid bare 'ref'\n",
        "ONE_LIRA_RE = re.compile(r'\\b1[.,]00\\s*tl?\\b')      # 1,00 / 1.00 TL"
      ],
      "metadata": {
        "id": "N9ltoEfh7xYk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_cluster_type(line_texts):\n",
        "    full = _norm(\" \".join(line_texts))\n",
        "\n",
        "    # 1) Keyword hits per type (normalize keywords too)\n",
        "    base_scores, per_type_hits = {}, {}\n",
        "    for t, kws in RECEIPT_TYPE_KEYWORDS_NORM.items():\n",
        "        hits = sum(1 for kw in kws if kw in full)  # each kw max 1\n",
        "        per_type_hits[t] = hits\n",
        "        base_scores[t] = float(hits)\n",
        "\n",
        "    # 2) Structural boosts\n",
        "    has_amt  = bool(AMOUNT_RE.search(full)) or bool(ONE_LIRA_RE.search(full))\n",
        "    has_date = bool(DATE_RE.search(full))\n",
        "    has_time = bool(TIME_RE.search(full))\n",
        "    has_onay = (\"onay numarasi\" in full) or (\"onay kodu\" in full) or (\"onay\" in full)\n",
        "    has_ref  = bool(REFNO_RE.search(full))\n",
        "\n",
        "    # i:/t: only count if they actually carry a number (reduces noise)\n",
        "    has_i_tag = bool(I_COLON_RE.search(full))\n",
        "    has_t_tag = bool(T_COLON_RE.search(full))\n",
        "\n",
        "    w_amt, w_date, w_time, w_onay, w_ref, w_tags = 0.7, 0.4, 0.3, 0.5, 0.4, 0.25\n",
        "    struct_score = (w_amt*has_amt + w_date*has_date + w_time*has_time +\n",
        "                    w_onay*has_onay + w_ref*has_ref +\n",
        "                    w_tags*(has_i_tag or has_t_tag))\n",
        "\n",
        "    scores = base_scores.copy()\n",
        "    for t in (\"satis\", \"iptal\"):\n",
        "        scores[t] += struct_score\n",
        "\n",
        "    # 4) Conflict rules\n",
        "    if \"iptal\" in full or \"satis iptal\" in full:\n",
        "        scores[\"satis\"] -= 0.8\n",
        "\n",
        "    if per_type_hits.get(\"gunsonu\", 0) >= 2:\n",
        "        scores[\"satis\"] -= 0.5; scores[\"iptal\"] -= 0.5; scores[\"parametre\"] -= 0.3\n",
        "\n",
        "    if per_type_hits.get(\"parametre\", 0) >= 2:\n",
        "        scores[\"satis\"] -= 0.5; scores[\"iptal\"] -= 0.5; scores[\"gunsonu\"] -= 0.3\n",
        "\n",
        "    # 5) Decide\n",
        "    best_type = max(scores, key=scores.get)\n",
        "    best_score = scores[best_type]\n",
        "    delta = 0.6\n",
        "    tags = [t for t, sc in scores.items() if sc >= best_score - delta and sc > 0]\n",
        "\n",
        "    confidence = max(0.0, min(1.0, best_score / 4.0))\n",
        "\n",
        "    return {\n",
        "        \"type\": best_type if best_score > 0 else None,\n",
        "        \"scores\": scores,\n",
        "        \"confidence\": round(confidence, 3),\n",
        "        \"tags\": sorted(tags, key=lambda t: -scores[t]),\n",
        "    }"
      ],
      "metadata": {
        "id": "BaJQzDyFO_oL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group_boxes_into_lines(boxes, y_threshold=20):\n",
        "    \"\"\"Groups word-boxes into lines by vertical proximity.\"\"\"\n",
        "    if not boxes:\n",
        "        return []\n",
        "    sorted_boxes = sorted(boxes, key=lambda b: b[\"center\"][1])\n",
        "    lines, current = [], [sorted_boxes[0]]\n",
        "    for b in sorted_boxes[1:]:\n",
        "        if abs(b[\"center\"][1] - current[-1][\"center\"][1]) <= y_threshold:\n",
        "            current.append(b)\n",
        "        else:\n",
        "            lines.append(current)\n",
        "            current = [b]\n",
        "    lines.append(current)\n",
        "    return lines"
      ],
      "metadata": {
        "id": "qTMuY0Kzmt5-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _first_match(text: str, patterns):\n",
        "    for p in patterns:\n",
        "        m = re.search(p, text)\n",
        "        if m:\n",
        "            return m\n",
        "    return None\n",
        "\n",
        "def extract_fields_from_text_lines(lines, image_path):\n",
        "    results = {}\n",
        "    filename = os.path.basename(image_path)\n",
        "    m = re.search(r\"(\\d+)\", filename)  # first number in filename\n",
        "    results[\"receipt_id\"] = m.group(1) if m else os.path.splitext(filename)[0]\n",
        "\n",
        "    bank_keywords = [\"GARANTI BBVA\",\"ISBANK\",\"ZIRAAT\",\"YAPI KREDI\",\n",
        "                     \"AKBANK\",\"HALKBANK\",\"QNB\",\"TEB\",\"DENIZBANK\"]\n",
        "\n",
        "    # collect potential Mali IDs (there can be more than one)\n",
        "    mali_ids = []\n",
        "\n",
        "    for raw in lines:\n",
        "        upper = normalize_ocr_text(raw)\n",
        "\n",
        "        # --- Date ---\n",
        "        m = re.search(r\"\\b(\\d{2}[./-]\\d{2}[./-](\\d{2,4}))\\b\", upper)\n",
        "        if m: results.setdefault(\"tarih\", m.group(1))\n",
        "\n",
        "        # --- Time ---\n",
        "        m = re.search(r\"\\b(\\d{2}[:.]\\d{2}(?::\\d{2})?)\\b\", upper)\n",
        "        if m: results.setdefault(\"saat\", m.group(1))\n",
        "\n",
        "        # --- İşyeri No (I:, ISYERI NO, ISYERI:) ---\n",
        "        m = _first_match(upper, [\n",
        "            r\"\\bISYERI\\s*NO[:\\s\\-]*([0-9]{5,})\\b\",\n",
        "            r\"\\bISYERI[:\\s\\-]*([0-9]{5,})\\b\",\n",
        "            r\"\\bI\\s*[:\\-]\\s*([0-9]{5,})\\b\",\n",
        "        ])\n",
        "        if m: results.setdefault(\"isyeri_no\", m.group(1))\n",
        "\n",
        "        # --- Terminal No (T:, TERMINAL NO, TERMINAL:) ---\n",
        "        m = _first_match(upper, [\n",
        "            r\"\\bTERMINAL\\s*NO[:\\s\\-]*([0-9]{4,})\\b\",\n",
        "            r\"\\bTERMINAL[:\\s\\-]*([0-9]{4,})\\b\",\n",
        "            r\"\\bT\\s*[:\\-]\\s*([0-9]{4,})\\b\",\n",
        "        ])\n",
        "        if m: results.setdefault(\"terminal_no\", m.group(1))\n",
        "\n",
        "        # --- MERSIS ---\n",
        "        m = re.search(r\"\\bMERSIS\\s*NO[:\\s]*([0-9\\s]{10,})\\b\", upper)\n",
        "        if m: results.setdefault(\"mersis_no\", m.group(1).replace(\" \",\"\"))\n",
        "\n",
        "        # --- VKN ---\n",
        "        m = re.search(r\"\\bVKN[:\\s]*([0-9]{5,})\\b\", upper)\n",
        "        if m: results.setdefault(\"vkn\", m.group(1))\n",
        "\n",
        "        # --- Ref No ---\n",
        "        m = re.search(r\"\\b(REF|REFERANS)\\s*(NO)?[:\\s]*([0-9]{5,})\\b\", upper)\n",
        "        if m: results.setdefault(\"ref_no\", m.group(3))\n",
        "\n",
        "        # --- Onay Numarasi / Code ---\n",
        "        m = _first_match(upper, [\n",
        "            r\"\\bONAY\\s*NUMARASI[:\\s]*([0-9]{5,})\\b\",\n",
        "            r\"\\bONAY\\s*KODU[:\\s]*([0-9]{5,})\\b\",\n",
        "        ])\n",
        "        if m: results.setdefault(\"onay_numarasi\", m.group(1))\n",
        "\n",
        "        # --- Amount (TL/TRY) ---\n",
        "        m = re.search(r\"\\b(\\d{1,4}[.,]\\d{2})\\s*(TL|TRY)\\b\", upper)\n",
        "        if m: results.setdefault(\"amount_tl\", m.group(1))\n",
        "\n",
        "        # --- Mali ID: AS / AT / AV + digits (allow spaces or hyphens) ---\n",
        "        for mi in re.finditer(r\"\\bA[STV][\\s\\-]*\\d{6,}\\b\", upper):\n",
        "            token = mi.group(0).replace(\" \", \"\").replace(\"-\", \"\")\n",
        "            mali_ids.append(token)\n",
        "\n",
        "        # --- Bank name ---\n",
        "        for bank in bank_keywords:\n",
        "            if bank in upper:\n",
        "                results.setdefault(\"bank\", bank)\n",
        "                break\n",
        "\n",
        "    if mali_ids and \"mali_id\" not in results:\n",
        "        # keep the longest or first; adjust as you prefer\n",
        "        results[\"mali_id\"] = max(mali_ids, key=len)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "8Dx806lDhu15"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_receipt_types(text, keyword_dict, threshold=1):\n",
        "    \"\"\"\n",
        "    Returns a list of receipt types that matched at least `threshold` keywords.\n",
        "    \"\"\"\n",
        "    normalized_text = text.lower()\n",
        "    matched_types = []\n",
        "\n",
        "    for receipt_type, keywords in keyword_dict.items():\n",
        "        count = sum(1 for kw in keywords if kw.lower() in normalized_text)\n",
        "        if count >= threshold:\n",
        "            matched_types.append(receipt_type)\n",
        "\n",
        "    return matched_types"
      ],
      "metadata": {
        "id": "B1BqGXVJPFOQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `is_receipt_image_from_path` — Purpose\n",
        "\n",
        "This is the **main OCR processing pipeline** for detecting and analyzing receipts from a given image file path.\n",
        "\n",
        "#### Why this is needed\n",
        "- It combines **image preprocessing**, **OCR text extraction**, **clustering**, **receipt scoring**, and **field/type detection** into one unified function.\n",
        "- Ensures the image is **correctly rotated** before OCR (important for Doctr accuracy).\n",
        "- Groups detected words into logical **receipt regions** for multi-receipt images.\n",
        "\n",
        "#### How it works\n",
        "1. **Image Loading & Preprocessing**\n",
        "   - Reads the image from disk.\n",
        "   - Enhances contrast using `apply_light_clahe`.\n",
        "   - Corrects rotation/skew using `correct_orientation_with_skew` based on keywords.\n",
        "\n",
        "2. **OCR with Doctr**\n",
        "   - Converts image to RGB and feeds it into the `doctr_model`.\n",
        "   - Extracts each word’s:\n",
        "     - Original text\n",
        "     - Normalized text\n",
        "     - Bounding box coordinates (absolute pixel values)\n",
        "     - Word center\n",
        "     - Confidence score\n",
        "\n",
        "3. **Adaptive Clustering**\n",
        "   - Uses `adaptive_dbscan_params` to dynamically calculate DBSCAN parameters (`eps_frac`, `min_samples`) based on word density and image size.\n",
        "   - Clusters words into potential receipts with `cluster_receipts_by_centroids`.\n",
        "\n",
        "4. **Per-Cluster Analysis**\n",
        "   - For each cluster:\n",
        "     - Computes bounding box (with padding).\n",
        "     - Groups words into lines (`group_boxes_into_lines`).\n",
        "     - Creates `line_texts` list for processing.\n",
        "     - Scores cluster using `compute_receipt_score` (keywords, amount, date, time).\n",
        "     - Extracts fields with `extract_fields_hybrid`.\n",
        "     - Detects receipt types with `detect_receipt_types`.\n",
        "     - Draws bounding boxes for visualization.\n",
        "\n",
        "5. **Final Decision**\n",
        "   - Marks image as containing a receipt if **any** cluster meets the `is_receipt` threshold.\n",
        "   - Returns:\n",
        "     - `receipts` → List of detected receipt regions with metadata\n",
        "     - `rotated_image` → Visualization with bounding boxes (optional)\n",
        "     - `words` → All OCR word objects\n",
        "     - `clusters` → Grouped words per receipt\n",
        "     - `is_receipt_image` → Boolean overall decision\n"
      ],
      "metadata": {
        "id": "nD7qDZ076xpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_receipt_image_from_path(image_path, doctr_model, return_rotated=True,\n",
        "                               eps_frac=None, min_samples=None):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Could not load image: {image_path}\")\n",
        "    img_clahe = apply_light_clahe(img)\n",
        "    rotated, _ = correct_orientation_with_skew(img_clahe, keywords=keywords_for_detection, lang='tur')\n",
        "    H, W = rotated.shape[:2]\n",
        "\n",
        "    # OCR with Doctr\n",
        "    rgb = cv2.cvtColor(rotated, cv2.COLOR_BGR2RGB)\n",
        "    with torch.no_grad():\n",
        "        result = doctr_model([rgb])\n",
        "    page = result.pages[0]\n",
        "\n",
        "    # collect words\n",
        "    words = []\n",
        "    for block in page.blocks:\n",
        "        for line in block.lines:\n",
        "            for w in line.words:\n",
        "                txt = w.value.strip()\n",
        "                norm = normalize_ocr_text(txt)\n",
        "                if not norm:\n",
        "                    continue\n",
        "                (x1n, y1n), (x2n, y2n) = w.geometry\n",
        "                x1, y1 = int(x1n * W), int(y1n * H)\n",
        "                x2, y2 = int(x2n * W), int(y2n * H)\n",
        "                words.append({\n",
        "                    \"text\": txt,\n",
        "                    \"normalized_text\": norm,\n",
        "                    \"box\": [x1, y1, x2, y2],\n",
        "                    \"center\": [(x1 + x2) // 2, (y1 + y2) // 2],\n",
        "                    \"confidence\": getattr(w, \"confidence\", -1),\n",
        "                })\n",
        "\n",
        "    # --- Adaptive DBSCAN parameters ---\n",
        "    if eps_frac is None or min_samples is None:\n",
        "        eps_frac, min_samples = adaptive_dbscan_params(words, H, W)\n",
        "\n",
        "    # cluster into receipts\n",
        "    clusters = cluster_receipts_by_centroids(words, (H, W), eps_frac=eps_frac, min_samples=min_samples)\n",
        "\n",
        "    results = []\n",
        "    vis = rotated.copy()\n",
        "    for cluster in clusters:\n",
        "        xs1 = [wd[\"box\"][0] for wd in cluster]\n",
        "        ys1 = [wd[\"box\"][1] for wd in cluster]\n",
        "        xs2 = [wd[\"box\"][2] for wd in cluster]\n",
        "        ys2 = [wd[\"box\"][3] for wd in cluster]\n",
        "        x1, y1, x2, y2 = min(xs1), min(ys1), max(xs2), max(ys2)\n",
        "\n",
        "        # padding\n",
        "        pad_x = int(0.05 * (x2 - x1))\n",
        "        pad_y = int(0.05 * (y2 - y1))\n",
        "        x1p, y1p = max(0, x1 - pad_x), max(0, y1 - pad_y)\n",
        "        x2p, y2p = min(W, x2 + pad_x), min(H, y2 + pad_y)\n",
        "\n",
        "        # lines for THIS cluster\n",
        "        lines = group_boxes_into_lines(cluster)\n",
        "        line_texts = [\" \".join(wd[\"normalized_text\"] for wd in line) for line in lines]\n",
        "\n",
        "        # score cluster as receipt / not receipt\n",
        "        score_info = compute_receipt_score(line_texts, keywords_for_detection, return_debug=True, print_debug=True)\n",
        "\n",
        "        # # regex + semantic\n",
        "        # regex_fields = extract_fields_from_text_lines(line_texts, image_path)\n",
        "        # sem_fields = extract_with_semantics(line_texts)\n",
        "        fields = extract_fields_hybrid(line_texts, image_path)\n",
        "\n",
        "        cluster_text = \" \".join(line_texts).lower()\n",
        "        type_tags = detect_receipt_types(cluster_text, RECEIPT_TYPE_KEYWORDS, threshold=1)\n",
        "\n",
        "\n",
        "        # draw\n",
        "        color = (0, 255, 0) if score_info[\"is_receipt\"] else (0, 165, 255)\n",
        "        cv2.rectangle(vis, (x1p, y1p), (x2p, y2p), color, 2)\n",
        "\n",
        "        results.append({\n",
        "            \"region\": (x1p, y1p, x2p, y2p),\n",
        "            \"fields\": fields,\n",
        "            \"words\": cluster,\n",
        "            \"lines\": line_texts,\n",
        "            \"score\": score_info[\"score\"],\n",
        "            \"kw_hits\": score_info[\"kw_hits\"],\n",
        "            \"has_amount\": score_info[\"has_amount\"],\n",
        "            \"has_date\": score_info[\"has_date\"],\n",
        "            \"has_time\": score_info[\"has_time\"],\n",
        "            \"is_receipt\": score_info[\"is_receipt\"],\n",
        "            \"receipt_types\": type_tags,\n",
        "        })\n",
        "\n",
        "    image_is_receipt = any(r[\"is_receipt\"] for r in results)\n",
        "\n",
        "    return {\n",
        "        \"receipts\": results,\n",
        "        \"rotated_image\": vis if return_rotated else None,\n",
        "        \"words\": words,\n",
        "        \"clusters\": clusters,\n",
        "        \"is_receipt_image\": image_is_receipt\n",
        "    }\n"
      ],
      "metadata": {
        "id": "nqa53Gkiol-4"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BANK_NAMES = [\n",
        "    \"Garanti BBVA\", \"İş Bankası\", \"Ziraat Bankası\", \"Akbank\", \"Halkbank\", \"Yapı Kredi\",\n",
        "    \"VakıfBank\", \"QNB Finansbank\", \"DenizBank\", \"TEB\", \"ING Bank\", \"HSBC\",\n",
        "]\n",
        "\n",
        "DIGIT_PATTERNS = {\n",
        "    \"isyeri_no\": r\"(?:\\bISYERI\\s*NO\\b|\\bISYERI\\b|(?:\\b[I1]\\b)\\s*[:\\-])\\s*[:\\-]?\\s*([0-9]{5,})\",\n",
        "    \"terminal_no\": r\"(?:\\bTERMINAL\\s*NO\\b|\\bTERMINAL\\b|(?:\\b[T7]\\b)\\s*[:\\-])\\s*[:\\-]?\\s*([0-9]{4,})\",\n",
        "    \"onay_numarasi\": r\"(?:\\bONAY(?:\\s*NO|\\s*NUMARASI|\\s*KODU)?\\b)\\s*[:\\-]?\\s*([0-9]{5,})\",\n",
        "    \"ref_no\": r\"(?:\\bREF(?:ERANS)?\\s*NO?\\b)\\s*[:\\-]?\\s*([0-9]{5,})\",\n",
        "    \"tarih\": r\"\\b(\\d{2}[./-]\\d{2}[./-]\\d{2,4})\\b\",\n",
        "    \"saat\": r\"\\b(\\d{2}[:.]\\d{2}(?::\\d{2})?)\\b\",\n",
        "    \"amount_tl\": r\"\\b(\\d{1,4}[.,]\\d{2})\\s*(?:TL|TRY)?\\b\",\n",
        "    \"mali_id\": r\"\\b(A[STV][\\s\\-]*\\d{6,})\\b\",\n",
        "}\n",
        "\n",
        "# (optional) synonyms used only to decide neighbors when labels are split across lines\n",
        "FIELD_QUERIES = {\n",
        "    \"isyeri_no\":     [\"İşyeri\", \"ISYERI\", \"I:\"],\n",
        "    \"terminal_no\":   [\"Terminal\", \"TERMINAL\", \"T:\"],\n",
        "    \"onay_numarasi\": [\"Onay numarası\", \"Onay kodu\", \"ONAY\"],\n",
        "    \"ref_no\":        [\"Ref no\", \"Referans no\", \"REF\", \"REFERANS\"],\n",
        "    \"tarih\":         [\"Tarih\", \"Date\"],\n",
        "    \"saat\":          [\"Saat\", \"Time\"],\n",
        "    \"amount_tl\":     [\"Tutar\", \"Toplam\", \"Amount\", \"İşlem tutarı\"],\n",
        "    \"mali_id\":       [\"Mali id\", \"AV\", \"AS\", \"AT\"],\n",
        "    \"bank\":          [\"Banka\", \"Bank\"],\n",
        "}"
      ],
      "metadata": {
        "id": "gh_qfDSMWZfz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic + Regex-Based Field Extraction\n",
        "\n",
        "This section combines **semantic search** with **label-anchored regexes**  \n",
        "to robustly extract key receipt fields while avoiding false matches.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. `_norm_line`\n",
        "- **Purpose:** Normalizes OCR text for consistent matching.\n",
        "- **Steps:**\n",
        "  1. Convert to uppercase.\n",
        "  2. Normalize Unicode (remove accents).\n",
        "  3. Collapse multiple spaces into one.\n",
        "  4. Trim leading/trailing spaces.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. `_search_with_anchors`\n",
        "- **Purpose:** Uses a field-specific regex **anchored to a label** (e.g., `\"İŞYERİ NO:\"`)  \n",
        "  to extract the correct value from nearby lines.\n",
        "- **Search order:**\n",
        "  1. **Best candidate line** (from semantic match).\n",
        "  2. 2 **neighboring lines**.\n",
        "  3. **Global scan** across all lines (still anchored to label for safety).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. `semantic_pick_best_line`\n",
        "- **Purpose:** Chooses the most relevant line for a given field using semantic similarity.\n",
        "- **Process:**\n",
        "  1. Embed all OCR lines and the field’s query list using `sem_model`.\n",
        "  2. Compute cosine similarity between queries and lines.\n",
        "  3. Pick the line with the highest similarity score.\n",
        "  4. Reject if below `min_score` (default `0.35`).\n",
        "\n",
        "---\n",
        "\n",
        "### 4. `extract_with_semantics`\n",
        "- **Purpose:** Full extraction pipeline for one receipt cluster.\n",
        "- **Steps:**\n",
        "  1. Normalize all lines with `_norm_line`.\n",
        "  2. **Bank name detection:** Simple substring match from a predefined `BANK_NAMES` list.\n",
        "  3. **Other fields:**\n",
        "     - Use `semantic_pick_best_line` to find the most relevant line.\n",
        "     - Extract value with `_search_with_anchors`.\n",
        "     - Apply field-specific post-processing (e.g., remove spaces/dashes for `mali_id`, length check for `terminal_no`).\n",
        "     - Save both the extracted value and its semantic similarity score.\n"
      ],
      "metadata": {
        "id": "yTz_xiBIGzKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _norm_line(s: str) -> str:\n",
        "    # same normalization you use elsewhere\n",
        "    s = s.upper()\n",
        "    s = unicodedata.normalize(\"NFKD\", s)\n",
        "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "def _search_with_anchors(field, lines_norm, start_idx):\n",
        "    \"\"\"Try the best line, then ±2 neighbors, then all lines using the field's anchored regex.\"\"\"\n",
        "    pat = re.compile(DIGIT_PATTERNS[field], re.IGNORECASE)\n",
        "    # 1) best line\n",
        "    if 0 <= start_idx < len(lines_norm):\n",
        "        m = pat.search(lines_norm[start_idx])\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "\n",
        "    # 2) neighborhood\n",
        "    for idx in range(max(0, start_idx-2), min(len(lines_norm), start_idx+3)):\n",
        "        if idx == start_idx:\n",
        "            continue\n",
        "        m = pat.search(lines_norm[idx])\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "\n",
        "    # 3) global (still anchored to label, so safe)\n",
        "    for idx in range(len(lines_norm)):\n",
        "        m = pat.search(lines_norm[idx])\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "\n",
        "    return None\n",
        "def semantic_pick_best_line(lines, queries, model, min_score=0.35):\n",
        "    if not lines: return None, 0.0\n",
        "    line_emb = model.encode(lines, convert_to_tensor=True, normalize_embeddings=True)\n",
        "    q_emb = model.encode(queries, convert_to_tensor=True, normalize_embeddings=True)\n",
        "    sims = util.cos_sim(q_emb, line_emb).max(dim=0).values\n",
        "    top_idx = int(torch.argmax(sims))\n",
        "    top_score = float(sims[top_idx])\n",
        "    if top_score < min_score:\n",
        "        return None, 0.0\n",
        "    return top_idx, top_score\n",
        "\n",
        "\n",
        "def extract_with_semantics(line_texts):\n",
        "    \"\"\"\n",
        "    line_texts: list[str] for ONE receipt (cluster)\n",
        "    Uses semantics to pick the *right* line per field,\n",
        "    then extracts with label-anchored regexes. Prevents\n",
        "    'ISYERI NO' from filling every field.\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    if not line_texts:\n",
        "        return out\n",
        "\n",
        "    # normalized lines for matching\n",
        "    lines_norm = [_norm_line(s) for s in line_texts]\n",
        "\n",
        "    # BANK: simple case-insensitive name scan across all lines\n",
        "    for ln in lines_norm:\n",
        "        for bank in BANK_NAMES:\n",
        "            if _norm_line(bank) in ln:\n",
        "                out[\"bank\"] = bank\n",
        "                break\n",
        "        if \"bank\" in out:\n",
        "            break\n",
        "\n",
        "    # all other fields via semantic selection + anchored regex\n",
        "    for field, queries in FIELD_QUERIES.items():\n",
        "        if field == \"bank\":\n",
        "            continue\n",
        "\n",
        "        # 1) semantic: pick the most likely line index\n",
        "        best_idx, score = semantic_pick_best_line(line_texts, queries, sem_model, min_score=0.35)\n",
        "        if best_idx is None:\n",
        "            continue\n",
        "\n",
        "        # use index directly; no .index(...) re-search\n",
        "        val = _search_with_anchors(field, lines_norm, best_idx)\n",
        "\n",
        "        if not val:\n",
        "            continue\n",
        "\n",
        "        # post-process some fields\n",
        "        if field == \"mali_id\":\n",
        "            val = val.replace(\" \", \"\").replace(\"-\", \"\")\n",
        "        if field == \"amount_tl\":\n",
        "            # keep just the numeric part (already captured as group 1)\n",
        "            pass\n",
        "        if field == \"terminal_no\":\n",
        "            # sanity: terminal numbers are usually short (4–8)\n",
        "            if not (4 <= len(val) <= 8):\n",
        "                continue\n",
        "\n",
        "        out[field] = val\n",
        "        out[field + \"_score\"] = round(score, 3)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "-TKGCfli_y92"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_fields_hybrid(line_texts, image_path):\n",
        "    \"\"\"\n",
        "    Run regex-based extraction first, then fill missing values using semantic model.\n",
        "    \"\"\"\n",
        "    regex_results = extract_fields_from_text_lines(line_texts, image_path)\n",
        "    semantic_results = extract_with_semantics(line_texts)\n",
        "\n",
        "    final = regex_results.copy()\n",
        "    for k, v in semantic_results.items():\n",
        "        if k.endswith(\"_score\"):\n",
        "            continue\n",
        "        if not final.get(k):  # only fill if missing\n",
        "            final[k] = v\n",
        "            final[k + \"_score\"] = semantic_results.get(k + \"_score\", None)\n",
        "    return final\n"
      ],
      "metadata": {
        "id": "see7U2Gvw0iJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fields we expect to be UNIQUE per image (take consensus across clusters)\n",
        "UNIQUE_FIELDS = [\n",
        "    \"isyeri_no\", \"terminal_no\", \"onay_numarasi\", \"ref_no\",\n",
        "    \"tarih\", \"saat\", \"bank\", \"vkn\", \"mersis_no\", \"mali_id\"\n",
        "]\n",
        "\n",
        "AMOUNT_FIELD = \"amount_tl\""
      ],
      "metadata": {
        "id": "CsK-22JE8fo1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- CONFIG ----------\n",
        "IMAGE_FOLDER = \"/content/drive/MyDrive/deneme\"\n",
        "OUTPUT_CSV   = \"receipts_output.csv\"\n",
        "ALLOWED_EXTS = (\"*.jpg\", \"*.jpeg\", \"*.png\")\n",
        "# ---------------------------"
      ],
      "metadata": {
        "id": "_3yJfZZaFWwZ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Receipt Field Consolidation Logic\n",
        "\n",
        "This code block is responsible for **post-processing** and **consolidating** extracted receipt data after OCR and clustering.  \n",
        "Its purpose is to merge multiple detections from the same image into a **single, clean record**.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. `iter_image_paths`\n",
        "- **Function:** Collects all image file paths from a folder that match allowed extensions.\n",
        "- **Why:** Ensures the pipeline processes the correct set of receipt images.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. `_normalize_token`\n",
        "- **Function:** Trims spaces, collapses multiple spaces into one, and converts text to uppercase.\n",
        "- **Why:** Makes string comparisons **case-insensitive** and **space-insensitive** to help with majority voting.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. `_choose_by_majority`\n",
        "- **Function:** Picks the most common (majority) value from multiple detections.\n",
        "- **Process:**\n",
        "  1. Normalize values using `_normalize_token`.\n",
        "  2. Count occurrences of each normalized value.\n",
        "  3. If there is a tie:\n",
        "     - Use **cluster scores** (higher wins).\n",
        "     - If no scores, pick the first occurrence.\n",
        "- **Why:** OCR may produce different values for the same field across clusters; this ensures the most reliable value is chosen.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. `_uniq_preserve_order`\n",
        "- **Function:** Removes duplicates from a list while preserving original order.\n",
        "- **Why:** Useful for combining all detected amounts without changing their sequence.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. `consolidate_receipt_fields`\n",
        "- **Function:** Merges all extracted data from multiple receipt clusters into a **final, unified record** for one image.\n",
        "- **Steps:**\n",
        "  1. **Unique fields (e.g., `isyeri_no`, `terminal_no`, `bank`):**\n",
        "     - Apply `_choose_by_majority` to select the best value.\n",
        "  2. **Derive numeric summaries:**\n",
        "     - Convert textual amounts into floats (handling Turkish decimal separators)."
      ],
      "metadata": {
        "id": "Z7UQtTTeGaHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iter_image_paths(folder, exts=ALLOWED_EXTS):\n",
        "    paths = []\n",
        "    for ext in exts:\n",
        "        paths += glob.glob(os.path.join(folder, ext))\n",
        "    return sorted(paths)\n",
        "\n",
        "def _normalize_token(s):\n",
        "    if s is None:\n",
        "        return None\n",
        "    s = str(s).strip()\n",
        "    # collapse spaces & uppercase (helps majority vote)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).upper()\n",
        "    return s\n",
        "\n",
        "def _choose_by_majority(values, scores=None):\n",
        "    \"\"\"\n",
        "    values: list of strings (may include None or '')\n",
        "    scores: optional list of numeric scores (same length) — e.g., cluster scores\n",
        "    Rule:\n",
        "      1) Majority vote on normalized token\n",
        "      2) Tie-break by best score (if provided) else first occurrence\n",
        "    Returns the original (un-normalized) value chosen.\n",
        "    \"\"\"\n",
        "    # map normalized -> list of (orig_value, idx)\n",
        "    buckets = {}\n",
        "    for i, v in enumerate(values):\n",
        "        if not v:\n",
        "            continue\n",
        "        norm = _normalize_token(v)\n",
        "        if not norm:\n",
        "            continue\n",
        "        buckets.setdefault(norm, []).append((v, i))\n",
        "\n",
        "    if not buckets:\n",
        "        return None\n",
        "\n",
        "    # majority size\n",
        "    counts = {k: len(vs) for k, vs in buckets.items()}\n",
        "    best_norm = max(counts.keys(), key=lambda k: counts[k])\n",
        "    tied_norms = [k for k,c in counts.items() if c == counts[best_norm]]\n",
        "\n",
        "    if len(tied_norms) == 1 or not scores:\n",
        "        # single winner or no scores -> take first original in that bucket\n",
        "        return buckets[tied_norms[0]][0][0]\n",
        "\n",
        "    # tie-break with scores (higher better)\n",
        "    best = None\n",
        "    best_score = float(\"-inf\")\n",
        "    for norm in tied_norms:\n",
        "        for (orig, idx) in buckets[norm]:\n",
        "            sc = scores[idx] if idx < len(scores) and scores[idx] is not None else 0.0\n",
        "            if sc > best_score:\n",
        "                best_score = sc\n",
        "                best = orig\n",
        "    return best"
      ],
      "metadata": {
        "id": "7QzZ30QrGYCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _uniq_preserve_order(seq):\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for x in seq:\n",
        "        if x in seen:\n",
        "            continue\n",
        "        seen.add(x)\n",
        "        out.append(x)\n",
        "    return out\n",
        "\n",
        "def consolidate_receipt_fields(receipts):\n",
        "    \"\"\"\n",
        "    receipts: list of cluster dicts from is_receipt_image_from_path\n",
        "              each has .fields, .score, .lines, etc.\n",
        "    Returns a single dict of consolidated fields for ONE image.\n",
        "    \"\"\"\n",
        "    # gather cluster scores for tie-breaking\n",
        "    cluster_scores = [r.get(\"score\") for r in receipts]\n",
        "\n",
        "    consolidated = {}\n",
        "\n",
        "    # 1) unique fields: majority vote across clusters\n",
        "    for field in UNIQUE_FIELDS:\n",
        "        vals = []\n",
        "        for r in receipts:\n",
        "            val = (r.get(\"fields\") or {}).get(field)\n",
        "            vals.append(val)\n",
        "        consolidated[field] = _choose_by_majority(vals, scores=cluster_scores)\n",
        "\n",
        "    # 2) amounts: collect all amounts we found across clusters (dedup)\n",
        "    all_amounts = []\n",
        "    for r in receipts:\n",
        "        # if your extractor put amount into fields:\n",
        "        f = (r.get(\"fields\") or {}).get(AMOUNT_FIELD)\n",
        "        if f:\n",
        "            all_amounts.append(str(f))\n",
        "\n",
        "        # optional: scan lines to catch amounts missed by extractor\n",
        "        for ln in r.get(\"lines\") or []:\n",
        "            for m in re.finditer(r\"\\b(\\d{1,4}[.,]\\d{2})\\s*(TL|TRY)?\\b\", ln.upper()):\n",
        "                all_amounts.append(m.group(1))\n",
        "\n",
        "    amounts_uniq = _uniq_preserve_order([a.replace(\" \", \"\") for a in all_amounts])\n",
        "    consolidated[\"amounts_all\"] = \";\".join(amounts_uniq)\n",
        "\n",
        "    # derive max/total numeric amounts (best-effort)\n",
        "    def _parse_amount(a):\n",
        "        # convert \"1.234,56\" or \"1234,56\" or \"1234.56\" to float\n",
        "        a = a.replace(\" \", \"\")\n",
        "        if a.count(\",\") == 1 and a.count(\".\") >= 1:\n",
        "            # assume thousand sep '.' and decimal ','\n",
        "            a = a.replace(\".\", \"\").replace(\",\", \".\")\n",
        "        elif a.count(\",\") == 1 and a.count(\".\") == 0:\n",
        "            a = a.replace(\",\", \".\")\n",
        "        try:\n",
        "            return float(a)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    nums = [ _parse_amount(a) for a in amounts_uniq ]\n",
        "    nums = [x for x in nums if x is not None]\n",
        "    consolidated[\"amount_max\"] = max(nums) if nums else None\n",
        "    consolidated[\"amount_sum\"] = round(sum(nums), 2) if nums else None\n",
        "\n",
        "    return consolidated"
      ],
      "metadata": {
        "id": "3YXeP8AQGBbd"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "\n",
        "for img_path in iter_image_paths(IMAGE_FOLDER):\n",
        "    try:\n",
        "        out = is_receipt_image_from_path(img_path, doctr_model, return_rotated=False)\n",
        "\n",
        "        receipts = out.get(\"receipts\", [])\n",
        "        image_is_receipt = any(r.get(\"is_receipt\") for r in receipts) if receipts else False\n",
        "\n",
        "        # consolidate even if no clusters (returns mostly None)\n",
        "        if receipts:\n",
        "            consolidated = consolidate_receipt_fields(receipts)\n",
        "        else:\n",
        "            consolidated = {k: None for k in UNIQUE_FIELDS}\n",
        "            consolidated.update({\"amounts_all\":\"\", \"amount_max\":None, \"amount_sum\":None})\n",
        "\n",
        "        # you can also union receipt types per image if you already compute them in your pipeline\n",
        "        types_union = sorted({t for r in receipts for t in r.get(\"receipt_types\", [])})\n",
        "        row = {\n",
        "            \"image_path\": img_path,\n",
        "            \"num_clusters\": len(receipts),\n",
        "            \"is_receipt_image\": image_is_receipt,\n",
        "            \"receipt_types\": \";\".join(types_union) if types_union else \"\",\n",
        "        }\n",
        "        # add consolidated unique fields + amounts\n",
        "        row.update(consolidated)\n",
        "\n",
        "        rows.append(row)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {img_path}: {e}\")\n",
        "        rows.append({\n",
        "            \"image_path\": img_path,\n",
        "            \"num_clusters\": 0,\n",
        "            \"is_receipt_image\": False,\n",
        "            \"receipt_types\": \"\",\n",
        "            **{k: None for k in UNIQUE_FIELDS},\n",
        "            \"amounts_all\": \"\",\n",
        "            \"amount_max\": None,\n",
        "            \"amount_sum\": None,\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "print(f\"Saved {len(df)} image rows to {OUTPUT_CSV}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "cnLtPXuXGhL0",
        "outputId": "8fa10c61-c24b-4022-8ce2-48acfb4af206",
        "collapsed": true
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4091621551.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_image_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_FOLDER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_receipt_image_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_rotated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mreceipts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"receipts\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2658872550.py\u001b[0m in \u001b[0;36mis_receipt_image_from_path\u001b[0;34m(image_path, doctr_model, return_rotated, eps_frac, min_samples)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not load image: {image_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimg_clahe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_light_clahe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mrotated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_orientation_with_skew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_clahe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeywords_for_detection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tur'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrotated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1269778630.py\u001b[0m in \u001b[0;36mcorrect_orientation_with_skew\u001b[0;34m(image, keywords, lang, skew_range, step, top_n_angles)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \"\"\"\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# Step 1: Try 0°, 90°, 180°, 270°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mupright_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_angle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best_orientation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# Step 2: Estimate fine skew angle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1269778630.py\u001b[0m in \u001b[0;36mfind_best_orientation\u001b[0;34m(image, keywords, lang)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mangle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m180\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m270\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mrotated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrotate_image_precise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytesseract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDICT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_avg_conf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mimage_to_data\u001b[0;34m(image, lang, config, nice, output_type, timeout, pandas_config)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m     return {\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBYTES\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         Output.DATAFRAME: lambda: get_pandas_output(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mpandas_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         ),\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDICT\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfile_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTRING\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     }[output_type]()\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mrun_and_get_output\u001b[0;34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[0m\n\u001b[1;32m    350\u001b[0m         }\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mrun_tesseract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         return _read_output(\n\u001b[1;32m    354\u001b[0m             \u001b[0;34mf\"{kwargs['output_filename_base']}{extsep}{extension}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mrun_tesseract\u001b[0;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTesseractNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtimeout_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror_string\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTesseractError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mtimeout_manager\u001b[0;34m(proc, seconds)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zIYQtMdKFZPY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}