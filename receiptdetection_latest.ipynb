{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## tesseract - doctr setup"
      ],
      "metadata": {
        "id": "Tc3rEZzeXRxe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xrgLjqtUTQLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40655c20-cc35-497c-b776-d808b678e34b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesseract OCR setup...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Tesseract setup done.\n",
            "Python libraries\n",
            "\u001b[33mWARNING: python-doctr 1.0.0 does not provide the extra 'torch'\u001b[0m\u001b[33m\n",
            "\u001b[0mLibrary set up done.\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "35 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr-tur is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# Tesseract OCR\n",
        "print(\"Tesseract OCR setup...\")\n",
        "!apt-get update > /dev/null\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-tur > /dev/null\n",
        "print(\"Tesseract setup done.\")\n",
        "\n",
        "print(\"Python libraries\")\n",
        "!pip install \"python-doctr[torch]\" pytesseract tqdm -q\n",
        "print(\"Library set up done.\")\n",
        "\n",
        "# other required dependencies\n",
        "!pip install opencv-python-headless pytesseract numpy matplotlib\n",
        "!sudo apt update\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-tur"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "BMJDEZgVShfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pytesseract\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import glob\n",
        "import torch\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from typing import List, Dict\n",
        "from pytesseract import image_to_data, Output\n",
        "from tqdm.notebook import tqdm\n",
        "from skimage.filters import threshold_sauvola\n",
        "from scipy.ndimage import rotate\n",
        "from difflib import get_close_matches\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files, drive  # For file uploads and Google Drive mounting\n",
        "from google.colab.patches import cv2_imshow  # For displaying images in Colab\n",
        "import cv2\n",
        "# from skimage.filters import threshold_sauvola\n",
        "from PIL import Image\n",
        "# Import doctr for OCR\n",
        "from doctr.io import DocumentFile\n",
        "from doctr.models import ocr_predictor, db_resnet50, detection_predictor\n",
        "import tempfile\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from sklearn.cluster import DBSCAN\n",
        "print(\"Installed dependencies.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsrsoZj9TUPY",
        "outputId": "afcdfa2c-1d95-4016-997c-94370f38b3dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed dependencies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG9wGYQATWJi",
        "outputId": "cbbf2a7b-05ed-4dfc-897f-4350a98fcfef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **`load_doctr_model()`** loads the Doctr OCR model **(ocr_predictor)** with detection architecture **DB-ResNet50** and recognition architecture **CRNN-VGG16-BN**.\n",
        "- The model is set to evaluation mode and returned for use in OCR tasks."
      ],
      "metadata": {
        "id": "Us8IYqOESVI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_doctr_model():\n",
        "    \"\"\"\n",
        "    Load the Doctr OCR model.\n",
        "\n",
        "    Returns:\n",
        "        model (doctr.models.ocr_predictor): The loaded OCR predictor model in evaluation mode.\n",
        "                                             Returns None if loading fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        _device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        model = ocr_predictor(\n",
        "            pretrained=True,\n",
        "            det_arch='db_resnet50',\n",
        "            reco_arch='crnn_vgg16_bn'\n",
        "        ).to(_device)\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        print(f\"Doctr OCR model loaded on {_device}.\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Doctr model could not be loaded: {e}. OCR functionality will be limited.\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "JS1AsVb_U9BM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doctr_model = load_doctr_model()"
      ],
      "metadata": {
        "id": "_kFqzfQejy5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce4d1349-fc2c-464b-97f5-11938f95381d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doctr OCR model loaded on cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sem_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D75c1027Us1",
        "outputId": "94711cc3-69bb-4903-9d36-7f26be70abd8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# keywords for the receipt detection\n",
        "keywords_for_detection = [\n",
        "    'TOPLAM', 'TUTAR', 'KDV', 'TL', 'FATURA', 'TARİH', 'SAAT', 'İŞLEM', 'MÜŞTERİ', 'BAŞARILI',\n",
        "    'SATIŞ', 'TUTARI', 'YERMİNAL', 'GARANTİ', 'ONAY', 'MERSIS', 'KART', 'BANKA', 'NO', 'ADET',\n",
        "    'ISLEM', 'PARAMETRE', 'YUKLEME', 'TERMİNAL', 'ISYERİ', 'TERMINAL', 'BATCH', 'GARANTI BBVA',\n",
        "    'DENIZBANK', 'TOPKDV', '1,00 TL', 'SATIŞ TUTARI', 'İŞLEM TUTARI','IPTAL','İPTAL','ISYERI'\n",
        "]"
      ],
      "metadata": {
        "id": "j6RvEGQyU_e4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This module below makes sure each receipt image is upright and deskewed before OCR (Doctr). It first fixes coarse rotation (0/90/180/270), then corrects small tilt, and optionally fine-refines alignment."
      ],
      "metadata": {
        "id": "4KVuOhuEZS2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_image_precise(image, angle, background_color=(255, 255, 255)):\n",
        "    \"\"\"\n",
        "    Why used:\n",
        "      - Apply arbitrary-angle rotation WITHOUT cropping, so no text is lost.\n",
        "      - Used for both coarse (0/90/180/270) and fine skew corrections before OCR.\n",
        "    \"\"\"\n",
        "    if angle == 0: return image\n",
        "    h, w = image.shape[:2]\n",
        "    center = (w // 2, h // 2)\n",
        "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    cos, sin = np.abs(M[0, 0]), np.abs(M[0, 1])\n",
        "    nW, nH = int((h * sin) + (w * cos)), int((h * cos) + (w * sin))\n",
        "    M[0, 2] += (nW / 2) - center[0]\n",
        "    M[1, 2] += (nH / 2) - center[1]\n",
        "    return cv2.warpAffine(image, M, (nW, nH), flags=cv2.INTER_CUBIC,\n",
        "                          borderMode=cv2.BORDER_CONSTANT, borderValue=background_color)\n",
        "\n",
        "def estimate_skew_hough(image):\n",
        "    \"\"\"\n",
        "    Why used:\n",
        "      - After coarse rotation, receipts may still be slightly tilted.\n",
        "      - Detects small skew via Hough lines and returns a robust median angle.\n",
        "    \"\"\"\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    inverted = cv2.bitwise_not(gray)\n",
        "    thresh = cv2.adaptiveThreshold(inverted, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                   cv2.THRESH_BINARY, 15, 2)\n",
        "    dilated = cv2.dilate(thresh, np.ones((3, 3), np.uint8), iterations=1)\n",
        "    edges = cv2.Canny(dilated, 50, 150)\n",
        "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 150,\n",
        "                            minLineLength=100, maxLineGap=20)\n",
        "    angles = []\n",
        "    if lines is not None:\n",
        "        for x1, y1, x2, y2 in lines[:, 0]:\n",
        "            if x2 == x1: continue\n",
        "            angle = np.degrees(np.arctan2(y2 - y1, x2 - x1))\n",
        "            if -45 < angle < 45:\n",
        "                angles.append(angle)\n",
        "    return np.median(angles) if angles else 0\n",
        "\n",
        "def projection_variance_score(img):\n",
        "    \"\"\"\n",
        "    Why used:\n",
        "      - Cheap numeric proxy for “how straight” text lines are.\n",
        "      - Enables micro-refinement by comparing nearby angles (higher = better).\n",
        "    \"\"\"\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    inverted = cv2.bitwise_not(thresh)\n",
        "    proj = np.sum(inverted, axis=1)\n",
        "    return np.var(proj)\n",
        "\n",
        "def compute_avg_conf(data):\n",
        "    \"\"\"\n",
        "    Why used:\n",
        "      - Orientation selection needs an OCR quality signal.\n",
        "      - Average token confidence is a stable metric to compare rotations.\n",
        "    \"\"\"\n",
        "    confs = []\n",
        "    for txt, conf in zip(data['text'], data['conf']):\n",
        "        try:\n",
        "            val = float(conf)\n",
        "            if txt.strip() and 0 < val < 100:\n",
        "                confs.append(val)\n",
        "        except:\n",
        "            continue\n",
        "    return np.mean(confs) if confs else 0\n",
        "\n",
        "def count_keywords(text, keywords):\n",
        "    \"\"\"\n",
        "    Why used:\n",
        "      - Receipts contain domain-specific tokens (TOPLAM, ONAY, etc.).\n",
        "      - Acts as a prior to break ties when OCR confidences are close.\n",
        "    \"\"\"\n",
        "    text = text.upper()\n",
        "    return sum(text.count(k.upper()) for k in keywords)\n",
        "\n",
        "def find_best_orientation(image, keywords, lang='tur'):\n",
        "    \"\"\"\n",
        "    Why used:\n",
        "      - Doctr is rotation-sensitive; choose best of 0/90/180/270 FIRST.\n",
        "      - Scores each angle by (keyword hits + OCR confidence), with a small angle penalty,\n",
        "        then returns the top candidate to feed into skew correction.\n",
        "    \"\"\"\n",
        "    best_img = image\n",
        "    best_angle = 0\n",
        "    best_score = -1\n",
        "    best_conf = 0\n",
        "    best_kw_count = 0\n",
        "\n",
        "    angle_penalty = {\n",
        "        0: 1.0,\n",
        "        90: 0.85,\n",
        "        180: 0.7,\n",
        "        270: 0.85\n",
        "    }\n",
        "\n",
        "    for angle in [0, 90, 180, 270]:\n",
        "        rotated = rotate_image_precise(image, angle)\n",
        "        data = pytesseract.image_to_data(rotated, lang=lang, output_type=Output.DICT)\n",
        "        conf = compute_avg_conf(data)\n",
        "        text = \" \".join(data['text'])\n",
        "        kw_count = count_keywords(text, keywords)\n",
        "\n",
        "        # Apply penalty to prefer natural upright when signals tie\n",
        "        base_score = kw_count * 1000 + conf\n",
        "        final_score = base_score * angle_penalty.get(angle, 0.7)\n",
        "\n",
        "        if final_score > best_score:\n",
        "            best_score = final_score\n",
        "            best_img = rotated\n",
        "            best_angle = angle\n",
        "            best_conf = conf\n",
        "            best_kw_count = kw_count\n",
        "\n",
        "    return best_img, best_angle\n",
        "\n",
        "def correct_orientation_with_skew(image, keywords=None, lang='tur+eng',\n",
        "                                  skew_range=5, step=1.0, top_n_angles=3):\n",
        "    \"\"\"\n",
        "    Why used:\n",
        "      - Single entry point to produce a Doctr-ready image.\n",
        "      - Pipeline: coarse rotation → small skew estimation → optional micro-refine.\n",
        "      - Returns final image + total angle for audit/reproducibility.\n",
        "    \"\"\"\n",
        "    # Step 1: Try 0°, 90°, 180°, 270°\n",
        "    upright_img, base_angle = find_best_orientation(image, keywords, lang=lang)\n",
        "\n",
        "    # Step 2: Estimate fine skew angle\n",
        "    skew_angle = estimate_skew_hough(upright_img)\n",
        "    corrected_img = rotate_image_precise(upright_img, skew_angle)\n",
        "\n",
        "    # Step 3: Optionally refine skew using projection variance\n",
        "    # print(\"Refining skew...\")\n",
        "    angles = np.arange(skew_angle - skew_range, skew_angle + skew_range + step, step)\n",
        "    scored_angles = []\n",
        "    for a in angles:\n",
        "        r = rotate_image_precise(upright_img, a)\n",
        "        score = projection_variance_score(r)\n",
        "        scored_angles.append((a, score))\n",
        "\n",
        "    top_angles = sorted(scored_angles, key=lambda x: -x[1])[:top_n_angles]\n",
        "    best_conf = compute_avg_conf(pytesseract.image_to_data(corrected_img, lang=lang, output_type=Output.DICT))\n",
        "    best_img = corrected_img\n",
        "    best_angle = base_angle + skew_angle\n",
        "\n",
        "    for a, _ in top_angles:\n",
        "        rotated = rotate_image_precise(upright_img, a)\n",
        "        data = pytesseract.image_to_data(rotated, lang=lang, output_type=Output.DICT)\n",
        "        conf = compute_avg_conf(data)\n",
        "        if conf > best_conf:\n",
        "            best_conf = conf\n",
        "            best_img = rotated\n",
        "            best_angle = base_angle + a\n",
        "\n",
        "    return best_img, best_angle"
      ],
      "metadata": {
        "id": "8Aab8Y4mZFHP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `apply_light_clahe(image_bgr)`\n",
        "**Why used:** Enhance text visibility and contrast without over-saturating the image, especially in low-contrast or faded receipts.  \n",
        "**How it works:**\n",
        "1. **Convert to LAB:** Separates luminance (L) from color channels (A, B).  \n",
        "2. **CLAHE on L-channel:** Applies *Contrast Limited Adaptive Histogram Equalization* with a low clip limit (≤3.0) to subtly boost local contrast.  \n",
        "3. **Merge & Convert:** Combines adjusted luminance with original color and converts back to BGR.  \n",
        "\n",
        "**Returns:** A contrast-enhanced BGR image, ready for further preprocessing or OCR."
      ],
      "metadata": {
        "id": "zujqzFIBZd2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_light_clahe(image_bgr):\n",
        "    # Convert to LAB color space\n",
        "    lab = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "\n",
        "    # Apply CLAHE to the L-channel\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))  # clipLimit < 3.0 is \"light\"\n",
        "    cl = clahe.apply(l)\n",
        "\n",
        "    # Merge and convert back\n",
        "    merged = cv2.merge((cl, a, b))\n",
        "    enhanced = cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    return enhanced"
      ],
      "metadata": {
        "id": "5xWjGCM3TWcF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `cluster_receipts_by_centroids` — Purpose\n",
        "\n",
        "This function groups detected OCR words into **clusters** (likely representing individual receipts)  \n",
        "based on their **center coordinates** in the image.  \n",
        "It uses **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) to find  \n",
        "groups of nearby words without needing to predefine the number of clusters.\n",
        "\n",
        "#### Why this is needed\n",
        "- A single image can contain **multiple receipts**.\n",
        "- We want to group words that **belong to the same receipt**.\n",
        "- DBSCAN is ideal because it:\n",
        "  - Clusters based on **spatial proximity** (word positions).\n",
        "  - Can handle **noise/outliers** (unwanted words outside receipts).\n",
        "  - Does not require specifying the number of receipts beforehand.\n",
        "\n",
        "#### How it works\n",
        "1. Takes the **center points** of all OCR-detected words.\n",
        "2. Converts `eps_frac` into an actual pixel distance using the image diagonal.\n",
        "3. Runs DBSCAN to find clusters.\n",
        "4. Returns each cluster as a list of word dictionaries.\n"
      ],
      "metadata": {
        "id": "3B7w5s4l58WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_receipts_by_centroids(words, image_shape, eps_frac=0.05, min_samples=5):\n",
        "    \"\"\"\n",
        "    words: list of dicts, each with 'center':(cx,cy), and 'box':(x1,y1,x2,y2)\n",
        "    image_shape: (h,w) of the image\n",
        "    Returns: List of clusters, each a list of word dicts\n",
        "    \"\"\"\n",
        "    if not words:\n",
        "        return []\n",
        "\n",
        "    # build matrix of (cx, cy)\n",
        "    X = np.array([w['center'] for w in words], dtype=float)\n",
        "    # eps = fraction of image diagonal\n",
        "    h, w = image_shape\n",
        "    eps = np.hypot(w, h) * eps_frac\n",
        "\n",
        "    db = DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
        "    labels = db.labels_\n",
        "\n",
        "    clusters = []\n",
        "    for lab in sorted(set(labels)):\n",
        "        if lab < 0:  # noise\n",
        "            continue\n",
        "        idxs = np.where(labels == lab)[0]\n",
        "        cluster = [words[i] for i in idxs]\n",
        "        clusters.append(cluster)\n",
        "    return clusters"
      ],
      "metadata": {
        "id": "PMsJwmZdR3ES"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `compute_receipt_score` — Purpose\n",
        "\n",
        "This function below calculates a **confidence score** to decide if a given cluster of OCR lines is likely to be a **receipt**.\n",
        "\n",
        "#### Why this is needed\n",
        "- After clustering OCR words into potential receipts, we still need to **validate** if a cluster is truly a receipt.\n",
        "- A high keyword match alone may not be enough; receipts also have **amounts, dates, and times**.\n",
        "- This scoring approach gives a **weighted evaluation** instead of a simple yes/no check.\n",
        "\n",
        "#### How it works\n",
        "1. **Combine all text** from the cluster into a single string.\n",
        "2. **Normalize text** for consistent matching (e.g., removing accents, fixing OCR inconsistencies).\n",
        "3. **Count keyword matches** from `keywords_for_detection`.\n",
        "4. **Detect patterns** using regex\n",
        "5. **Calculate weighted score**:\n",
        "6. **Return results** with:\n",
        "   - Score\n",
        "   - Pattern presence booleans\n",
        "   - Final `is_receipt` flag (score ≥ threshold)."
      ],
      "metadata": {
        "id": "EEkrbSd56QLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def compute_receipt_score(line_texts, keywords_for_detection, kw_weight=0.4,\n",
        "#                           amt_weight=0.6, date_weight=0.5, time_weight=0.4,\n",
        "#                           threshold=1.8):\n",
        "#     \"\"\"\n",
        "#     line_texts: list[str] (one cluster's lines, already normalized if you want)\n",
        "#     keywords_for_detection: list[str]\n",
        "#     returns: dict with score, booleans, and is_receipt\n",
        "#     \"\"\"\n",
        "#     # flatten text for keyword counting\n",
        "#     full = \" \".join(line_texts)\n",
        "#     full_norm = normalize_ocr_text(full)\n",
        "\n",
        "#     # keyword hits (count)\n",
        "#     kw_hits = sum(full_norm.count(normalize_ocr_text(k)) for k in keywords_for_detection)\n",
        "\n",
        "#     # regex signals\n",
        "#     has_amount = bool(re.search(r'\\d+[.,]\\d{2}\\s*(TL|TRY)?', full_norm))\n",
        "#     has_date   = bool(re.search(r'\\b\\d{2}[./-]\\d{2}[./-]\\d{2,4}\\b', full_norm))\n",
        "#     has_time   = bool(re.search(r'\\b\\d{2}[:.]\\d{2}(?::\\d{2})?\\b', full_norm))\n",
        "\n",
        "#     # score (same scheme you used before)\n",
        "#     score = (kw_weight * kw_hits +\n",
        "#              amt_weight * has_amount +\n",
        "#              date_weight * has_date +\n",
        "#              time_weight * has_time)\n",
        "\n",
        "#     return {\n",
        "#         \"score\": round(score, 3),\n",
        "#         \"kw_hits\": int(kw_hits),\n",
        "#         \"has_amount\": bool(has_amount),\n",
        "#         \"has_date\": bool(has_date),\n",
        "#         \"has_time\": bool(has_time),\n",
        "#         \"is_receipt\": score >= threshold,\n",
        "#     }"
      ],
      "metadata": {
        "id": "YXIi1z6sKf4p"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_receipt_score(\n",
        "    line_texts,\n",
        "    keywords_for_detection,\n",
        "    kw_weight=0.4,\n",
        "    amt_weight=0.6,\n",
        "    date_weight=0.5,\n",
        "    time_weight=0.4,\n",
        "    threshold=1.8,\n",
        "    return_debug=False,\n",
        "    print_debug=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    line_texts: list[str] (one cluster's lines)\n",
        "    keywords_for_detection: list[str]\n",
        "    Returns: dict with score, booleans, is_receipt, and (optionally) debug info.\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    # 1) Flatten + normalize once\n",
        "    full = \" \".join(line_texts or [])\n",
        "    full_norm = normalize_ocr_text(full)\n",
        "\n",
        "    # 2) Keyword hits (count) + which keywords matched\n",
        "    norm_kws = [normalize_ocr_text(k) for k in (keywords_for_detection or [])]\n",
        "    matched_kws = sorted({k for k in norm_kws if k and k in full_norm})\n",
        "    kw_hits = sum(full_norm.count(k) for k in matched_kws)\n",
        "\n",
        "    # 3) Regex signals (+ capture examples)\n",
        "    amount_re = re.compile(r'\\b\\d+[.,]\\d{2}\\s*(?:TL|TRY)?\\b')\n",
        "    date_re   = re.compile(r'\\b\\d{2}[./-]\\d{2}[./-]\\d{2,4}\\b')\n",
        "    time_re   = re.compile(r'\\b\\d{2}[:.]\\d{2}(?::\\d{2})?\\b')\n",
        "\n",
        "    amount_matches = amount_re.findall(full_norm)\n",
        "    date_matches   = date_re.findall(full_norm)\n",
        "    time_matches   = time_re.findall(full_norm)\n",
        "\n",
        "    has_amount = bool(amount_matches)\n",
        "    has_date   = bool(date_matches)\n",
        "    has_time   = bool(time_matches)\n",
        "\n",
        "    # 4) Score (with visible component contributions)\n",
        "    kw_contrib   = kw_weight  * kw_hits\n",
        "    amt_contrib  = amt_weight * int(has_amount)\n",
        "    date_contrib = date_weight* int(has_date)\n",
        "    time_contrib = time_weight* int(has_time)\n",
        "\n",
        "    score = kw_contrib + amt_contrib + date_contrib + time_contrib\n",
        "    is_receipt = score >= threshold\n",
        "\n",
        "    result = {\n",
        "        \"score\": round(score, 3),\n",
        "        \"kw_hits\": int(kw_hits),\n",
        "        \"has_amount\": bool(has_amount),\n",
        "        \"has_date\": bool(has_date),\n",
        "        \"has_time\": bool(has_time),\n",
        "        \"is_receipt\": is_receipt,\n",
        "    }\n",
        "\n",
        "    if return_debug:\n",
        "        result[\"debug\"] = {\n",
        "            \"text_preview\": full_norm[:300],\n",
        "            \"matched_keywords\": matched_kws,\n",
        "            \"amount_matches\": amount_matches[:10],\n",
        "            \"date_matches\": date_matches[:10],\n",
        "            \"time_matches\": time_matches[:10],\n",
        "            \"weights\": {\n",
        "                \"kw_weight\": kw_weight,\n",
        "                \"amt_weight\": amt_weight,\n",
        "                \"date_weight\": date_weight,\n",
        "                \"time_weight\": time_weight,\n",
        "                \"threshold\": threshold,\n",
        "            },\n",
        "            \"components\": {\n",
        "                \"kw_contrib\": round(kw_contrib, 3),\n",
        "                \"amt_contrib\": round(amt_contrib, 3),\n",
        "                \"date_contrib\": round(date_contrib, 3),\n",
        "                \"time_contrib\": round(time_contrib, 3),\n",
        "            },\n",
        "        }\n",
        "\n",
        "    if print_debug:\n",
        "        print(\n",
        "            f\"[receipt_score] score={score:.3f} (kw={kw_contrib:.2f}, amt={amt_contrib:.2f}, \"\n",
        "            f\"date={date_contrib:.2f}, time={time_contrib:.2f}) | \"\n",
        "            f\"kw_hits={kw_hits} | amount={has_amount} date={has_date} time={has_time} | \"\n",
        "            f\"thr={threshold} => is_receipt={is_receipt}\"\n",
        "        )\n",
        "        if matched_kws:\n",
        "            print(\"  matched_keywords:\", matched_kws)\n",
        "        if amount_matches or date_matches or time_matches:\n",
        "            print(\"  samples:\",\n",
        "                  {\"amount\": amount_matches[:3], \"date\": date_matches[:3], \"time\": time_matches[:3]})\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "m95kkMe6ELuI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adaptive_dbscan_params(words, H, W):\n",
        "    # Image diagonal\n",
        "    diag = (H**2 + W**2) ** 0.5\n",
        "    n_words = len(words)\n",
        "\n",
        "    if n_words == 0:\n",
        "        return 0.08, 5  # fallback\n",
        "\n",
        "    # Estimate spacing from word centers\n",
        "    centers = np.array([w[\"center\"] for w in words])\n",
        "    if len(centers) > 1:\n",
        "        from sklearn.neighbors import NearestNeighbors\n",
        "        nbrs = NearestNeighbors(n_neighbors=2).fit(centers)\n",
        "        distances, _ = nbrs.kneighbors(centers)\n",
        "        avg_dist = np.mean(distances[:, 1])\n",
        "    else:\n",
        "        avg_dist = diag * 0.05\n",
        "\n",
        "    # eps fraction based on spacing (clamp between 0.05 and 0.15)\n",
        "    eps_frac = min(0.15, max(0.05, avg_dist / diag * 1.5))\n",
        "\n",
        "    # min_samples based on total word count\n",
        "    if n_words < 30:\n",
        "        min_samples = 3\n",
        "    elif n_words < 80:\n",
        "        min_samples = 5\n",
        "    else:\n",
        "        min_samples = 8\n",
        "\n",
        "    return eps_frac, min_samples\n"
      ],
      "metadata": {
        "id": "qCODJJn5oqni"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Keep your existing dict but lowercase all entries\n",
        "RECEIPT_TYPE_KEYWORDS = {\n",
        "    \"gunsonu\": [\n",
        "        \"günsonu raporu\",\"günsonu işlemi\",\"rapor başlangıcı\",\"rapor sonu\",\n",
        "        \"başarılı olarak tamamlanmıştır\",\"gönderim raporu\",\"genel toplam\",\n",
        "        \"peşin iptal\",\"peşin ipt\",\"batch no\"\n",
        "    ],\n",
        "    \"satis\": [\n",
        "        \"satış\",\"satış tutarı\",\"işlem tutarı\",\"kredi kartı\",\"onay numarası\",\n",
        "        \"onay kodu\",\"kart sahibine aittir\",\"topkdv\",\"toplam\",\"1,00\",\"1,00tl\",\"i:\",\"t:\"\n",
        "    ],\n",
        "    \"iptal\": [\n",
        "        \"satış iptal\",\"satış iptal tutarı\",\"iptal\",\"ref no\",\"onay numarası\",\n",
        "        \"tarih\",\"1,00tl\",\"kart sahibine aittir\",\"i:\",\"t:\"\n",
        "    ],\n",
        "    \"parametre\": [\n",
        "        \"parametre yükleme\",\"parametre\",\"host\",\"tid\",\"mid\",\"os ver\",\n",
        "        \"başlangıç\",\"bitiş\",\"pos aktivasyon başarılı\",\"key exchange başarılı\"\n",
        "    ],\n",
        "    \"detay_listesi\": [\n",
        "        \"detay işlemler listesi\",\"grup 1 işlem sayısı\",\"grup başarılı\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "AMOUNT_RE = re.compile(r'\\b\\d{1,4}[.,]\\d{2}\\s*(?:TL|TRY)?\\b')\n",
        "DATE_RE   = re.compile(r'\\b\\d{2}[./-]\\d{2}[./-]\\d{2,4}\\b')\n",
        "TIME_RE   = re.compile(r'\\b\\d{2}[:.]\\d{2}(?::\\d{2})?\\b')"
      ],
      "metadata": {
        "id": "AdPs9blMcAX_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `score_cluster_type` — Purpose\n",
        "\n",
        "This function determines the **most likely receipt type** for a given cluster of OCR-extracted lines by combining **keyword-based** and **structural** pattern analysis.\n",
        "\n",
        "#### Why this is needed\n",
        "- A single image can contain **different receipt formats** (e.g., *satış*, *iptal*, *günsonu*, *parametre*).\n",
        "- We need a **type classification step** so downstream processing can apply type-specific extraction rules.\n",
        "- This approach uses **both text keywords** and **receipt-structure patterns** for higher accuracy.\n",
        "\n",
        "#### How it works\n",
        "1. **Text Normalization**\n",
        "   - Converts all text to lowercase, removes accents, and collapses spaces for consistent matching.\n",
        "\n",
        "2. **Keyword Hit Counting**\n",
        "   - For each known receipt type (from `RECEIPT_TYPE_KEYWORDS_NORM`), counts how many of its normalized keywords appear in the cluster.\n",
        "\n",
        "3. **Structural Pattern Boosts**\n",
        "   - Checks for additional receipt-specific patterns:\n",
        "     - Amounts (e.g., `1,00 TL`)\n",
        "     - Dates\n",
        "     - Approval codes (\"Onay numarası\")\n",
        "     - Reference numbers\n",
        "     - Special tags like `I:` or `T:` with digits.\n",
        "   - Adds weighted boosts to *satış* and *iptal* types based on these patterns.\n",
        "\n",
        "4. **Conflict Resolution Rules**\n",
        "   - Penalizes *satış* score if \"iptal\" appears.\n",
        "   - Penalizes *satış*, *iptal*, and *parametre* if *günsonu* signals dominate.\n",
        "   - Penalizes *satış*, *iptal*, and *günsonu* if *parametre* signals dominate.\n",
        "\n",
        "5. **Final Decision**\n",
        "   - Selects the highest-scoring type as `best_type`.\n",
        "   - Also lists all plausible `tags` within a score delta (`delta = 0.6`).\n",
        "   - Returns:\n",
        "     - `type` → most probable receipt type\n",
        "     - `scores` → score per type\n",
        "     - `confidence` → normalized confidence (0-1)\n",
        "     - `tags` → all candidate types above the threshold"
      ],
      "metadata": {
        "id": "XekYCxDl6eoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_ocr_text(text: str) -> str:\n",
        "    # 1) Uppercase\n",
        "    text = text.upper()\n",
        "    # 2) Remove accents (İ -> I, Ş -> S, etc.)\n",
        "    text = unicodedata.normalize(\"NFKD\", text)\n",
        "    text = \"\".join([c for c in text if not unicodedata.combining(c)])\n",
        "    # 3) Replace multiple spaces with single\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    # 4) Strip edges\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "M5Izg-Fo-yjL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _norm(s: str) -> str:\n",
        "    # your normalize_ocr_text: upper + strip accents + collapse spaces\n",
        "    s = normalize_ocr_text(s)\n",
        "    return s.lower()\n",
        "\n",
        "# Build a normalized copy ONCE at import time\n",
        "RECEIPT_TYPE_KEYWORDS_NORM = {\n",
        "    t: [_norm(kw) for kw in kws]\n",
        "    for t, kws in RECEIPT_TYPE_KEYWORDS.items()\n",
        "}\n",
        "\n",
        "I_COLON_RE = re.compile(r'\\b[i1]\\s*:\\s*\\d+')   # I:/1: followed by digits\n",
        "T_COLON_RE = re.compile(r'\\b[t7]\\s*:\\s*\\d+')   # T:/7: followed by digits\n",
        "REFNO_RE   = re.compile(r'\\bref(?:erans)?\\s*no\\b')  # avoid bare 'ref'\n",
        "ONE_LIRA_RE = re.compile(r'\\b1[.,]00\\s*tl?\\b')      # 1,00 / 1.00 TL"
      ],
      "metadata": {
        "id": "N9ltoEfh7xYk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_cluster_type(line_texts):\n",
        "    full = _norm(\" \".join(line_texts))\n",
        "\n",
        "    # 1) Keyword hits per type (normalize keywords too)\n",
        "    base_scores, per_type_hits = {}, {}\n",
        "    for t, kws in RECEIPT_TYPE_KEYWORDS_NORM.items():\n",
        "        hits = sum(1 for kw in kws if kw in full)  # each kw max 1\n",
        "        per_type_hits[t] = hits\n",
        "        base_scores[t] = float(hits)\n",
        "\n",
        "    # 2) Structural boosts\n",
        "    has_amt  = bool(AMOUNT_RE.search(full)) or bool(ONE_LIRA_RE.search(full))\n",
        "    has_date = bool(DATE_RE.search(full))\n",
        "    has_time = bool(TIME_RE.search(full))\n",
        "    has_onay = (\"onay numarasi\" in full) or (\"onay kodu\" in full) or (\"onay\" in full)\n",
        "    has_ref  = bool(REFNO_RE.search(full))\n",
        "\n",
        "    # i:/t: only count if they actually carry a number (reduces noise)\n",
        "    has_i_tag = bool(I_COLON_RE.search(full))\n",
        "    has_t_tag = bool(T_COLON_RE.search(full))\n",
        "\n",
        "    w_amt, w_date, w_time, w_onay, w_ref, w_tags = 0.7, 0.4, 0.3, 0.5, 0.4, 0.25\n",
        "    struct_score = (w_amt*has_amt + w_date*has_date + w_time*has_time +\n",
        "                    w_onay*has_onay + w_ref*has_ref +\n",
        "                    w_tags*(has_i_tag or has_t_tag))\n",
        "\n",
        "    scores = base_scores.copy()\n",
        "    for t in (\"satis\", \"iptal\"):\n",
        "        scores[t] += struct_score\n",
        "\n",
        "    # 4) Conflict rules\n",
        "    if \"iptal\" in full or \"satis iptal\" in full:\n",
        "        scores[\"satis\"] -= 0.8\n",
        "\n",
        "    if per_type_hits.get(\"gunsonu\", 0) >= 2:\n",
        "        scores[\"satis\"] -= 0.5; scores[\"iptal\"] -= 0.5; scores[\"parametre\"] -= 0.3\n",
        "\n",
        "    if per_type_hits.get(\"parametre\", 0) >= 2:\n",
        "        scores[\"satis\"] -= 0.5; scores[\"iptal\"] -= 0.5; scores[\"gunsonu\"] -= 0.3\n",
        "\n",
        "    # 5) Decide\n",
        "    best_type = max(scores, key=scores.get)\n",
        "    best_score = scores[best_type]\n",
        "    delta = 0.6\n",
        "    tags = [t for t, sc in scores.items() if sc >= best_score - delta and sc > 0]\n",
        "\n",
        "    confidence = max(0.0, min(1.0, best_score / 4.0))\n",
        "\n",
        "    return {\n",
        "        \"type\": best_type if best_score > 0 else None,\n",
        "        \"scores\": scores,\n",
        "        \"confidence\": round(confidence, 3),\n",
        "        \"tags\": sorted(tags, key=lambda t: -scores[t]),\n",
        "    }"
      ],
      "metadata": {
        "id": "BaJQzDyFO_oL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group_boxes_into_lines(boxes, y_threshold=20):\n",
        "    \"\"\"Groups word-boxes into lines by vertical proximity.\"\"\"\n",
        "    if not boxes:\n",
        "        return []\n",
        "    sorted_boxes = sorted(boxes, key=lambda b: b[\"center\"][1])\n",
        "    lines, current = [], [sorted_boxes[0]]\n",
        "    for b in sorted_boxes[1:]:\n",
        "        if abs(b[\"center\"][1] - current[-1][\"center\"][1]) <= y_threshold:\n",
        "            current.append(b)\n",
        "        else:\n",
        "            lines.append(current)\n",
        "            current = [b]\n",
        "    lines.append(current)\n",
        "    return lines"
      ],
      "metadata": {
        "id": "qTMuY0Kzmt5-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _first_match(text: str, patterns):\n",
        "    for p in patterns:\n",
        "        m = re.search(p, text)\n",
        "        if m:\n",
        "            return m\n",
        "    return None\n",
        "\n",
        "def extract_fields_from_text_lines(lines, image_path):\n",
        "    results = {}\n",
        "    filename = os.path.basename(image_path)\n",
        "    m = re.search(r\"(\\d+)\", filename)  # first number in filename\n",
        "    results[\"receipt_id\"] = m.group(1) if m else os.path.splitext(filename)[0]\n",
        "\n",
        "    bank_keywords = [\"GARANTI BBVA\",\"ISBANK\",\"ZIRAAT\",\"YAPI KREDI\",\n",
        "                     \"AKBANK\",\"HALKBANK\",\"QNB\",\"TEB\",\"DENIZBANK\"]\n",
        "\n",
        "    # collect potential Mali IDs (there can be more than one)\n",
        "    mali_ids = []\n",
        "\n",
        "    for raw in lines:\n",
        "        upper = normalize_ocr_text(raw)\n",
        "\n",
        "        # --- Date ---\n",
        "        m = re.search(r\"\\b(\\d{2}[./-]\\d{2}[./-](\\d{2,4}))\\b\", upper)\n",
        "        if m: results.setdefault(\"tarih\", m.group(1))\n",
        "\n",
        "        # --- Time ---\n",
        "        m = re.search(r\"\\b(\\d{2}[:.]\\d{2}(?::\\d{2})?)\\b\", upper)\n",
        "        if m: results.setdefault(\"saat\", m.group(1))\n",
        "\n",
        "        # --- İşyeri No (I:, ISYERI NO, ISYERI:) ---\n",
        "        m = _first_match(upper, [\n",
        "            r\"\\bISYERI\\s*NO[:\\s\\-]*([0-9]{5,})\\b\",\n",
        "            r\"\\bISYERI[:\\s\\-]*([0-9]{5,})\\b\",\n",
        "            r\"\\bI\\s*[:\\-]\\s*([0-9]{5,})\\b\",\n",
        "        ])\n",
        "        if m: results.setdefault(\"isyeri_no\", m.group(1))\n",
        "\n",
        "        # --- Terminal No (T:, TERMINAL NO, TERMINAL:) ---\n",
        "        m = _first_match(upper, [\n",
        "            r\"\\bTERMINAL\\s*NO[:\\s\\-]*([0-9]{4,})\\b\",\n",
        "            r\"\\bTERMINAL[:\\s\\-]*([0-9]{4,})\\b\",\n",
        "            r\"\\bT\\s*[:\\-]\\s*([0-9]{4,})\\b\",\n",
        "        ])\n",
        "        if m: results.setdefault(\"terminal_no\", m.group(1))\n",
        "\n",
        "        # --- MERSIS ---\n",
        "        m = re.search(r\"\\bMERSIS\\s*NO[:\\s]*([0-9\\s]{10,})\\b\", upper)\n",
        "        if m: results.setdefault(\"mersis_no\", m.group(1).replace(\" \",\"\"))\n",
        "\n",
        "        # --- VKN ---\n",
        "        m = re.search(r\"\\bVKN[:\\s]*([0-9]{5,})\\b\", upper)\n",
        "        if m: results.setdefault(\"vkn\", m.group(1))\n",
        "\n",
        "        # --- Ref No ---\n",
        "        m = re.search(r\"\\b(REF|REFERANS)\\s*(NO)?[:\\s]*([0-9]{5,})\\b\", upper)\n",
        "        if m: results.setdefault(\"ref_no\", m.group(3))\n",
        "\n",
        "        # --- Onay Numarasi / Code ---\n",
        "        m = _first_match(upper, [\n",
        "            r\"\\bONAY\\s*NUMARASI[:\\s]*([0-9]{5,})\\b\",\n",
        "            r\"\\bONAY\\s*KODU[:\\s]*([0-9]{5,})\\b\",\n",
        "        ])\n",
        "        if m: results.setdefault(\"onay_numarasi\", m.group(1))\n",
        "\n",
        "        # --- Amount (TL/TRY) ---\n",
        "        m = re.search(r\"\\b(\\d{1,4}[.,]\\d{2})\\s*(TL|TRY)\\b\", upper)\n",
        "        if m: results.setdefault(\"amount_tl\", m.group(1))\n",
        "\n",
        "        # --- Mali ID: AS / AT / AV + digits (allow spaces or hyphens) ---\n",
        "        for mi in re.finditer(r\"\\bA[STV][\\s\\-]*\\d{6,}\\b\", upper):\n",
        "            token = mi.group(0).replace(\" \", \"\").replace(\"-\", \"\")\n",
        "            mali_ids.append(token)\n",
        "\n",
        "        # --- Bank name ---\n",
        "        for bank in bank_keywords:\n",
        "            if bank in upper:\n",
        "                results.setdefault(\"bank\", bank)\n",
        "                break\n",
        "\n",
        "    if mali_ids and \"mali_id\" not in results:\n",
        "        # keep the longest or first; adjust as you prefer\n",
        "        results[\"mali_id\"] = max(mali_ids, key=len)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "8Dx806lDhu15"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_receipt_types(text, keyword_dict, threshold=1):\n",
        "    \"\"\"\n",
        "    Returns a list of receipt types that matched at least `threshold` keywords.\n",
        "    \"\"\"\n",
        "    normalized_text = text.lower()\n",
        "    matched_types = []\n",
        "\n",
        "    for receipt_type, keywords in keyword_dict.items():\n",
        "        count = sum(1 for kw in keywords if kw.lower() in normalized_text)\n",
        "        if count >= threshold:\n",
        "            matched_types.append(receipt_type)\n",
        "\n",
        "    return matched_types"
      ],
      "metadata": {
        "id": "B1BqGXVJPFOQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `is_receipt_image_from_path` — Purpose\n",
        "\n",
        "This is the **main OCR processing pipeline** for detecting and analyzing receipts from a given image file path.\n",
        "\n",
        "#### Why this is needed\n",
        "- It combines **image preprocessing**, **OCR text extraction**, **clustering**, **receipt scoring**, and **field/type detection** into one unified function.\n",
        "- Ensures the image is **correctly rotated** before OCR (important for Doctr accuracy).\n",
        "- Groups detected words into logical **receipt regions** for multi-receipt images.\n",
        "\n",
        "#### How it works\n",
        "1. **Image Loading & Preprocessing**\n",
        "   - Reads the image from disk.\n",
        "   - Enhances contrast using `apply_light_clahe`.\n",
        "   - Corrects rotation/skew using `correct_orientation_with_skew` based on keywords.\n",
        "\n",
        "2. **OCR with Doctr**\n",
        "   - Converts image to RGB and feeds it into the `doctr_model`.\n",
        "   - Extracts each word’s:\n",
        "     - Original text\n",
        "     - Normalized text\n",
        "     - Bounding box coordinates (absolute pixel values)\n",
        "     - Word center\n",
        "     - Confidence score\n",
        "\n",
        "3. **Adaptive Clustering**\n",
        "   - Uses `adaptive_dbscan_params` to dynamically calculate DBSCAN parameters (`eps_frac`, `min_samples`) based on word density and image size.\n",
        "   - Clusters words into potential receipts with `cluster_receipts_by_centroids`.\n",
        "\n",
        "4. **Per-Cluster Analysis**\n",
        "   - For each cluster:\n",
        "     - Computes bounding box (with padding).\n",
        "     - Groups words into lines (`group_boxes_into_lines`).\n",
        "     - Creates `line_texts` list for processing.\n",
        "     - Scores cluster using `compute_receipt_score` (keywords, amount, date, time).\n",
        "     - Extracts fields with `extract_fields_hybrid`.\n",
        "     - Detects receipt types with `detect_receipt_types`.\n",
        "     - Draws bounding boxes for visualization.\n",
        "\n",
        "5. **Final Decision**\n",
        "   - Marks image as containing a receipt if **any** cluster meets the `is_receipt` threshold.\n",
        "   - Returns:\n",
        "     - `receipts` → List of detected receipt regions with metadata\n",
        "     - `rotated_image` → Visualization with bounding boxes (optional)\n",
        "     - `words` → All OCR word objects\n",
        "     - `clusters` → Grouped words per receipt\n",
        "     - `is_receipt_image` → Boolean overall decision\n"
      ],
      "metadata": {
        "id": "nD7qDZ076xpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_receipt_image_from_path(image_path, doctr_model, return_rotated=True,\n",
        "                               eps_frac=None, min_samples=None):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Could not load image: {image_path}\")\n",
        "    img_clahe = apply_light_clahe(img)\n",
        "    rotated, _ = correct_orientation_with_skew(img_clahe, keywords=keywords_for_detection, lang='tur')\n",
        "    H, W = rotated.shape[:2]\n",
        "\n",
        "    # OCR with Doctr\n",
        "    rgb = cv2.cvtColor(rotated, cv2.COLOR_BGR2RGB)\n",
        "    with torch.no_grad():\n",
        "        result = doctr_model([rgb])\n",
        "    page = result.pages[0]\n",
        "\n",
        "    # collect words\n",
        "    words = []\n",
        "    for block in page.blocks:\n",
        "        for line in block.lines:\n",
        "            for w in line.words:\n",
        "                txt = w.value.strip()\n",
        "                norm = normalize_ocr_text(txt)\n",
        "                if not norm:\n",
        "                    continue\n",
        "                (x1n, y1n), (x2n, y2n) = w.geometry\n",
        "                x1, y1 = int(x1n * W), int(y1n * H)\n",
        "                x2, y2 = int(x2n * W), int(y2n * H)\n",
        "                words.append({\n",
        "                    \"text\": txt,\n",
        "                    \"normalized_text\": norm,\n",
        "                    \"box\": [x1, y1, x2, y2],\n",
        "                    \"center\": [(x1 + x2) // 2, (y1 + y2) // 2],\n",
        "                    \"confidence\": getattr(w, \"confidence\", -1),\n",
        "                })\n",
        "\n",
        "    # --- Adaptive DBSCAN parameters ---\n",
        "    if eps_frac is None or min_samples is None:\n",
        "        eps_frac, min_samples = adaptive_dbscan_params(words, H, W)\n",
        "\n",
        "    # cluster into receipts\n",
        "    clusters = cluster_receipts_by_centroids(words, (H, W), eps_frac=eps_frac, min_samples=min_samples)\n",
        "\n",
        "    results = []\n",
        "    vis = rotated.copy()\n",
        "    for cluster in clusters:\n",
        "        xs1 = [wd[\"box\"][0] for wd in cluster]\n",
        "        ys1 = [wd[\"box\"][1] for wd in cluster]\n",
        "        xs2 = [wd[\"box\"][2] for wd in cluster]\n",
        "        ys2 = [wd[\"box\"][3] for wd in cluster]\n",
        "        x1, y1, x2, y2 = min(xs1), min(ys1), max(xs2), max(ys2)\n",
        "\n",
        "        # padding\n",
        "        pad_x = int(0.05 * (x2 - x1))\n",
        "        pad_y = int(0.05 * (y2 - y1))\n",
        "        x1p, y1p = max(0, x1 - pad_x), max(0, y1 - pad_y)\n",
        "        x2p, y2p = min(W, x2 + pad_x), min(H, y2 + pad_y)\n",
        "\n",
        "        # lines for THIS cluster\n",
        "        lines = group_boxes_into_lines(cluster)\n",
        "        line_texts = [\" \".join(wd[\"normalized_text\"] for wd in line) for line in lines]\n",
        "\n",
        "        # score cluster as receipt / not receipt\n",
        "        score_info = compute_receipt_score(line_texts, keywords_for_detection, return_debug=True, print_debug=True)\n",
        "\n",
        "        # # regex + semantic\n",
        "        # regex_fields = extract_fields_from_text_lines(line_texts, image_path)\n",
        "        # sem_fields = extract_with_semantics(line_texts)\n",
        "        fields = extract_fields_hybrid(line_texts, image_path)\n",
        "\n",
        "        cluster_text = \" \".join(line_texts).lower()\n",
        "        type_tags = detect_receipt_types(cluster_text, RECEIPT_TYPE_KEYWORDS, threshold=1)\n",
        "\n",
        "\n",
        "        # draw\n",
        "        color = (0, 255, 0) if score_info[\"is_receipt\"] else (0, 165, 255)\n",
        "        cv2.rectangle(vis, (x1p, y1p), (x2p, y2p), color, 2)\n",
        "\n",
        "        results.append({\n",
        "            \"region\": (x1p, y1p, x2p, y2p),\n",
        "            \"fields\": fields,\n",
        "            \"words\": cluster,\n",
        "            \"lines\": line_texts,\n",
        "            \"score\": score_info[\"score\"],\n",
        "            \"kw_hits\": score_info[\"kw_hits\"],\n",
        "            \"has_amount\": score_info[\"has_amount\"],\n",
        "            \"has_date\": score_info[\"has_date\"],\n",
        "            \"has_time\": score_info[\"has_time\"],\n",
        "            \"is_receipt\": score_info[\"is_receipt\"],\n",
        "            \"receipt_types\": type_tags,\n",
        "        })\n",
        "\n",
        "    image_is_receipt = any(r[\"is_receipt\"] for r in results)\n",
        "\n",
        "    return {\n",
        "        \"receipts\": results,\n",
        "        \"rotated_image\": vis if return_rotated else None,\n",
        "        \"words\": words,\n",
        "        \"clusters\": clusters,\n",
        "        \"is_receipt_image\": image_is_receipt\n",
        "    }\n"
      ],
      "metadata": {
        "id": "nqa53Gkiol-4"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BANK_NAMES = [\n",
        "    \"Garanti BBVA\", \"İş Bankası\", \"Ziraat Bankası\", \"Akbank\", \"Halkbank\", \"Yapı Kredi\",\n",
        "    \"VakıfBank\", \"QNB Finansbank\", \"DenizBank\", \"TEB\", \"ING Bank\", \"HSBC\",\n",
        "]\n",
        "\n",
        "DIGIT_PATTERNS = {\n",
        "    \"isyeri_no\": r\"(?:\\bISYERI\\s*NO\\b|\\bISYERI\\b|(?:\\b[I1]\\b)\\s*[:\\-])\\s*[:\\-]?\\s*([0-9]{5,})\",\n",
        "    \"terminal_no\": r\"(?:\\bTERMINAL\\s*NO\\b|\\bTERMINAL\\b|(?:\\b[T7]\\b)\\s*[:\\-])\\s*[:\\-]?\\s*([0-9]{4,})\",\n",
        "    \"onay_numarasi\": r\"(?:\\bONAY(?:\\s*NO|\\s*NUMARASI|\\s*KODU)?\\b)\\s*[:\\-]?\\s*([0-9]{5,})\",\n",
        "    \"ref_no\": r\"(?:\\bREF(?:ERANS)?\\s*NO?\\b)\\s*[:\\-]?\\s*([0-9]{5,})\",\n",
        "    \"tarih\": r\"\\b(\\d{2}[./-]\\d{2}[./-]\\d{2,4})\\b\",\n",
        "    \"saat\": r\"\\b(\\d{2}[:.]\\d{2}(?::\\d{2})?)\\b\",\n",
        "    \"amount_tl\": r\"\\b(\\d{1,4}[.,]\\d{2})\\s*(?:TL|TRY)?\\b\",\n",
        "    \"mali_id\": r\"\\b(A[STV][\\s\\-]*\\d{6,})\\b\",\n",
        "}\n",
        "\n",
        "# (optional) synonyms used only to decide neighbors when labels are split across lines\n",
        "FIELD_QUERIES = {\n",
        "    \"isyeri_no\":     [\"İşyeri\", \"ISYERI\", \"I:\"],\n",
        "    \"terminal_no\":   [\"Terminal\", \"TERMINAL\", \"T:\"],\n",
        "    \"onay_numarasi\": [\"Onay numarası\", \"Onay kodu\", \"ONAY\"],\n",
        "    \"ref_no\":        [\"Ref no\", \"Referans no\", \"REF\", \"REFERANS\"],\n",
        "    \"tarih\":         [\"Tarih\", \"Date\"],\n",
        "    \"saat\":          [\"Saat\", \"Time\"],\n",
        "    \"amount_tl\":     [\"Tutar\", \"Toplam\", \"Amount\", \"İşlem tutarı\"],\n",
        "    \"mali_id\":       [\"Mali id\", \"AV\", \"AS\", \"AT\"],\n",
        "    \"bank\":          [\"Banka\", \"Bank\"],\n",
        "}"
      ],
      "metadata": {
        "id": "gh_qfDSMWZfz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _norm_line(s: str) -> str:\n",
        "    # same normalization you use elsewhere\n",
        "    s = s.upper()\n",
        "    s = unicodedata.normalize(\"NFKD\", s)\n",
        "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "def _search_with_anchors(field, lines_norm, start_idx):\n",
        "    \"\"\"Try the best line, then ±2 neighbors, then all lines using the field's anchored regex.\"\"\"\n",
        "    pat = re.compile(DIGIT_PATTERNS[field], re.IGNORECASE)\n",
        "    # 1) best line\n",
        "    if 0 <= start_idx < len(lines_norm):\n",
        "        m = pat.search(lines_norm[start_idx])\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "\n",
        "    # 2) neighborhood\n",
        "    for idx in range(max(0, start_idx-2), min(len(lines_norm), start_idx+3)):\n",
        "        if idx == start_idx:\n",
        "            continue\n",
        "        m = pat.search(lines_norm[idx])\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "\n",
        "    # 3) global (still anchored to label, so safe)\n",
        "    for idx in range(len(lines_norm)):\n",
        "        m = pat.search(lines_norm[idx])\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "\n",
        "    return None\n",
        "def semantic_pick_best_line(lines, queries, model, min_score=0.35):\n",
        "    if not lines: return None, 0.0\n",
        "    line_emb = model.encode(lines, convert_to_tensor=True, normalize_embeddings=True)\n",
        "    q_emb = model.encode(queries, convert_to_tensor=True, normalize_embeddings=True)\n",
        "    sims = util.cos_sim(q_emb, line_emb).max(dim=0).values\n",
        "    top_idx = int(torch.argmax(sims))\n",
        "    top_score = float(sims[top_idx])\n",
        "    if top_score < min_score:\n",
        "        return None, 0.0\n",
        "    return top_idx, top_score\n",
        "\n",
        "\n",
        "def extract_with_semantics(line_texts):\n",
        "    \"\"\"\n",
        "    line_texts: list[str] for ONE receipt (cluster)\n",
        "    Uses semantics to pick the *right* line per field,\n",
        "    then extracts with label-anchored regexes. Prevents\n",
        "    'ISYERI NO' from filling every field.\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    if not line_texts:\n",
        "        return out\n",
        "\n",
        "    # normalized lines for matching\n",
        "    lines_norm = [_norm_line(s) for s in line_texts]\n",
        "\n",
        "    # BANK: simple case-insensitive name scan across all lines\n",
        "    for ln in lines_norm:\n",
        "        for bank in BANK_NAMES:\n",
        "            if _norm_line(bank) in ln:\n",
        "                out[\"bank\"] = bank\n",
        "                break\n",
        "        if \"bank\" in out:\n",
        "            break\n",
        "\n",
        "    # all other fields via semantic selection + anchored regex\n",
        "    for field, queries in FIELD_QUERIES.items():\n",
        "        if field == \"bank\":\n",
        "            continue\n",
        "\n",
        "        # 1) semantic: pick the most likely line index\n",
        "        best_idx, score = semantic_pick_best_line(line_texts, queries, sem_model, min_score=0.35)\n",
        "        if best_idx is None:\n",
        "            continue\n",
        "\n",
        "        # use index directly; no .index(...) re-search\n",
        "        val = _search_with_anchors(field, lines_norm, best_idx)\n",
        "\n",
        "        if not val:\n",
        "            continue\n",
        "\n",
        "        # post-process some fields\n",
        "        if field == \"mali_id\":\n",
        "            val = val.replace(\" \", \"\").replace(\"-\", \"\")\n",
        "        if field == \"amount_tl\":\n",
        "            # keep just the numeric part (already captured as group 1)\n",
        "            pass\n",
        "        if field == \"terminal_no\":\n",
        "            # sanity: terminal numbers are usually short (4–8)\n",
        "            if not (4 <= len(val) <= 8):\n",
        "                continue\n",
        "\n",
        "        out[field] = val\n",
        "        out[field + \"_score\"] = round(score, 3)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "-TKGCfli_y92"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_fields_hybrid(line_texts, image_path):\n",
        "    \"\"\"\n",
        "    Run regex-based extraction first, then fill missing values using semantic model.\n",
        "    \"\"\"\n",
        "    regex_results = extract_fields_from_text_lines(line_texts, image_path)\n",
        "    semantic_results = extract_with_semantics(line_texts)\n",
        "\n",
        "    final = regex_results.copy()\n",
        "    for k, v in semantic_results.items():\n",
        "        if k.endswith(\"_score\"):\n",
        "            continue\n",
        "        if not final.get(k):  # only fill if missing\n",
        "            final[k] = v\n",
        "            final[k + \"_score\"] = semantic_results.get(k + \"_score\", None)\n",
        "    return final\n"
      ],
      "metadata": {
        "id": "see7U2Gvw0iJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fields we expect to be UNIQUE per image (take consensus across clusters)\n",
        "UNIQUE_FIELDS = [\n",
        "    \"isyeri_no\", \"terminal_no\", \"onay_numarasi\", \"ref_no\",\n",
        "    \"tarih\", \"saat\", \"bank\", \"vkn\", \"mersis_no\", \"mali_id\"\n",
        "]\n",
        "\n",
        "AMOUNT_FIELD = \"amount_tl\""
      ],
      "metadata": {
        "id": "CsK-22JE8fo1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- CONFIG ----------\n",
        "IMAGE_FOLDER = \"/content/drive/MyDrive/denem1\"\n",
        "OUTPUT_CSV   = \"receipts_output.csv\"\n",
        "ALLOWED_EXTS = (\"*.jpg\", \"*.jpeg\", \"*.png\")\n",
        "# ---------------------------"
      ],
      "metadata": {
        "id": "_3yJfZZaFWwZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iter_image_paths(folder, exts=ALLOWED_EXTS):\n",
        "    paths = []\n",
        "    for ext in exts:\n",
        "        paths += glob.glob(os.path.join(folder, ext))\n",
        "    return sorted(paths)\n",
        "\n",
        "def _normalize_token(s):\n",
        "    if s is None:\n",
        "        return None\n",
        "    s = str(s).strip()\n",
        "    # collapse spaces & uppercase (helps majority vote)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).upper()\n",
        "    return s\n",
        "\n",
        "def _choose_by_majority(values, scores=None):\n",
        "    \"\"\"\n",
        "    values: list of strings (may include None or '')\n",
        "    scores: optional list of numeric scores (same length) — e.g., cluster scores\n",
        "    Rule:\n",
        "      1) Majority vote on normalized token\n",
        "      2) Tie-break by best score (if provided) else first occurrence\n",
        "    Returns the original (un-normalized) value chosen.\n",
        "    \"\"\"\n",
        "    # map normalized -> list of (orig_value, idx)\n",
        "    buckets = {}\n",
        "    for i, v in enumerate(values):\n",
        "        if not v:\n",
        "            continue\n",
        "        norm = _normalize_token(v)\n",
        "        if not norm:\n",
        "            continue\n",
        "        buckets.setdefault(norm, []).append((v, i))\n",
        "\n",
        "    if not buckets:\n",
        "        return None\n",
        "\n",
        "    # majority size\n",
        "    counts = {k: len(vs) for k, vs in buckets.items()}\n",
        "    best_norm = max(counts.keys(), key=lambda k: counts[k])\n",
        "    tied_norms = [k for k,c in counts.items() if c == counts[best_norm]]\n",
        "\n",
        "    if len(tied_norms) == 1 or not scores:\n",
        "        # single winner or no scores -> take first original in that bucket\n",
        "        return buckets[tied_norms[0]][0][0]\n",
        "\n",
        "    # tie-break with scores (higher better)\n",
        "    best = None\n",
        "    best_score = float(\"-inf\")\n",
        "    for norm in tied_norms:\n",
        "        for (orig, idx) in buckets[norm]:\n",
        "            sc = scores[idx] if idx < len(scores) and scores[idx] is not None else 0.0\n",
        "            if sc > best_score:\n",
        "                best_score = sc\n",
        "                best = orig\n",
        "    return best\n",
        "\n",
        "def _uniq_preserve_order(seq):\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for x in seq:\n",
        "        if x in seen:\n",
        "            continue\n",
        "        seen.add(x)\n",
        "        out.append(x)\n",
        "    return out\n",
        "\n",
        "def consolidate_receipt_fields(receipts):\n",
        "    \"\"\"\n",
        "    receipts: list of cluster dicts from is_receipt_image_from_path\n",
        "              each has .fields, .score, .lines, etc.\n",
        "    Returns a single dict of consolidated fields for ONE image.\n",
        "    \"\"\"\n",
        "    # gather cluster scores for tie-breaking\n",
        "    cluster_scores = [r.get(\"score\") for r in receipts]\n",
        "\n",
        "    consolidated = {}\n",
        "\n",
        "    # 1) unique fields: majority vote across clusters\n",
        "    for field in UNIQUE_FIELDS:\n",
        "        vals = []\n",
        "        for r in receipts:\n",
        "            val = (r.get(\"fields\") or {}).get(field)\n",
        "            vals.append(val)\n",
        "        consolidated[field] = _choose_by_majority(vals, scores=cluster_scores)\n",
        "\n",
        "    # 2) amounts: collect all amounts we found across clusters (dedup)\n",
        "    all_amounts = []\n",
        "    for r in receipts:\n",
        "        # if your extractor put amount into fields:\n",
        "        f = (r.get(\"fields\") or {}).get(AMOUNT_FIELD)\n",
        "        if f:\n",
        "            all_amounts.append(str(f))\n",
        "\n",
        "        # optional: scan lines to catch amounts missed by extractor\n",
        "        for ln in r.get(\"lines\") or []:\n",
        "            for m in re.finditer(r\"\\b(\\d{1,4}[.,]\\d{2})\\s*(TL|TRY)?\\b\", ln.upper()):\n",
        "                all_amounts.append(m.group(1))\n",
        "\n",
        "    amounts_uniq = _uniq_preserve_order([a.replace(\" \", \"\") for a in all_amounts])\n",
        "    consolidated[\"amounts_all\"] = \";\".join(amounts_uniq)\n",
        "\n",
        "    # Optional: derive max/total numeric amounts (best-effort)\n",
        "    def _parse_amount(a):\n",
        "        # convert \"1.234,56\" or \"1234,56\" or \"1234.56\" to float\n",
        "        a = a.replace(\" \", \"\")\n",
        "        if a.count(\",\") == 1 and a.count(\".\") >= 1:\n",
        "            # assume thousand sep '.' and decimal ','\n",
        "            a = a.replace(\".\", \"\").replace(\",\", \".\")\n",
        "        elif a.count(\",\") == 1 and a.count(\".\") == 0:\n",
        "            a = a.replace(\",\", \".\")\n",
        "        try:\n",
        "            return float(a)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    nums = [ _parse_amount(a) for a in amounts_uniq ]\n",
        "    nums = [x for x in nums if x is not None]\n",
        "    consolidated[\"amount_max\"] = max(nums) if nums else None\n",
        "    consolidated[\"amount_sum\"] = round(sum(nums), 2) if nums else None\n",
        "\n",
        "    return consolidated\n",
        "\n",
        "# ================= RUN OVER A FOLDER =================\n",
        "\n",
        "rows = []\n",
        "\n",
        "for img_path in iter_image_paths(IMAGE_FOLDER):\n",
        "    try:\n",
        "        out = is_receipt_image_from_path(img_path, doctr_model, return_rotated=False)\n",
        "\n",
        "        receipts = out.get(\"receipts\", [])\n",
        "        image_is_receipt = any(r.get(\"is_receipt\") for r in receipts) if receipts else False\n",
        "\n",
        "        # consolidate even if no clusters (returns mostly None)\n",
        "        if receipts:\n",
        "            consolidated = consolidate_receipt_fields(receipts)\n",
        "        else:\n",
        "            consolidated = {k: None for k in UNIQUE_FIELDS}\n",
        "            consolidated.update({\"amounts_all\":\"\", \"amount_max\":None, \"amount_sum\":None})\n",
        "\n",
        "        # you can also union receipt types per image if you already compute them in your pipeline\n",
        "        types_union = sorted({t for r in receipts for t in r.get(\"receipt_types\", [])})\n",
        "        row = {\n",
        "            \"image_path\": img_path,\n",
        "            \"num_clusters\": len(receipts),\n",
        "            \"is_receipt_image\": image_is_receipt,\n",
        "            \"receipt_types\": \";\".join(types_union) if types_union else \"\",\n",
        "        }\n",
        "        # add consolidated unique fields + amounts\n",
        "        row.update(consolidated)\n",
        "\n",
        "        rows.append(row)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {img_path}: {e}\")\n",
        "        rows.append({\n",
        "            \"image_path\": img_path,\n",
        "            \"num_clusters\": 0,\n",
        "            \"is_receipt_image\": False,\n",
        "            \"receipt_types\": \"\",\n",
        "            **{k: None for k in UNIQUE_FIELDS},\n",
        "            \"amounts_all\": \"\",\n",
        "            \"amount_max\": None,\n",
        "            \"amount_sum\": None,\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "print(f\"Saved {len(df)} image rows to {OUTPUT_CSV}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "cnLtPXuXGhL0",
        "outputId": "7c9d29c3-9159-4cfd-d1c0-dd258e4378d6",
        "collapsed": true
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-52139979.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_image_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_FOLDER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_receipt_image_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_rotated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mreceipts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"receipts\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2658872550.py\u001b[0m in \u001b[0;36mis_receipt_image_from_path\u001b[0;34m(image_path, doctr_model, return_rotated, eps_frac, min_samples)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not load image: {image_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimg_clahe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_light_clahe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mrotated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_orientation_with_skew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_clahe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeywords_for_detection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tur'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrotated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1269778630.py\u001b[0m in \u001b[0;36mcorrect_orientation_with_skew\u001b[0;34m(image, keywords, lang, skew_range, step, top_n_angles)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mtop_angles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscored_angles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_n_angles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mbest_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_avg_conf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytesseract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrected_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDICT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0mbest_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrected_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mbest_angle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_angle\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mskew_angle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mimage_to_data\u001b[0;34m(image, lang, config, nice, output_type, timeout, pandas_config)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m     return {\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBYTES\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         Output.DATAFRAME: lambda: get_pandas_output(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mpandas_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         ),\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDICT\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfile_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTRING\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     }[output_type]()\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mrun_and_get_output\u001b[0;34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mreturn_bytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m ):\n\u001b[0;32m--> 341\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtemp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         kwargs = {\n\u001b[1;32m    343\u001b[0m             \u001b[0;34m'input_filename'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0minput_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{f.name}_input{extsep}{extension}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_file_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2588\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2589\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2590\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mopen_fp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1493\u001b[0m         )\n\u001b[1;32m   1494\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msingle_im\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m         ImageFile._save(\n\u001b[0m\u001b[1;32m   1496\u001b[0m             \u001b[0msingle_im\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_idat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0m_encode_tile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0m_encode_tile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"flush\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    672\u001b[0m                     \u001b[0;31m# compress to Python file-compatible object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m                         \u001b[0merrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m                         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zIYQtMdKFZPY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}